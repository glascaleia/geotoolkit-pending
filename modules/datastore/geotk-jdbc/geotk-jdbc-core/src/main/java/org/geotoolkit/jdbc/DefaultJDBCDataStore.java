/*
 *    Geotoolkit - An Open Source Java GIS Toolkit
 *    http://www.geotoolkit.org
 *
 *    (C) 2002-2008, Open Source Geospatial Foundation (OSGeo)
 *    (C) 2009-2010, Geomatys
 *
 *    This library is free software; you can redistribute it and/or
 *    modify it under the terms of the GNU Lesser General Public
 *    License as published by the Free Software Foundation;
 *    version 2.1 of the License.
 *
 *    This library is distributed in the hope that it will be useful,
 *    but WITHOUT ANY WARRANTY; without even the implied warranty of
 *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 *    Lesser General Public License for more details.
 */
package org.geotoolkit.jdbc;

import org.geotoolkit.data.jdbc.PreparedFilterToSQL;
import com.vividsolutions.jts.geom.Geometry;

import java.io.IOException;
import java.io.UnsupportedEncodingException;
import java.math.BigDecimal;
import java.math.BigInteger;
import java.net.URLDecoder;
import java.sql.Connection;
import java.sql.DatabaseMetaData;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;
import java.sql.Types;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.logging.Level;

import org.geotoolkit.feature.SchemaException;
import org.geotoolkit.jdbc.fid.PrimaryKey;
import org.geotoolkit.jdbc.fid.PrimaryKeyColumn;
import org.geotoolkit.jdbc.fid.PrimaryKeyFIDValidator;
import org.geotoolkit.jdbc.fid.NullPrimaryKey;
import org.geotoolkit.jdbc.dialect.PreparedStatementSQLDialect;
import org.geotoolkit.jdbc.dialect.SQLDialect;
import org.geotoolkit.data.DataStoreException;
import org.geotoolkit.data.DataStoreRuntimeException;
import org.geotoolkit.data.FeatureReader;
import org.geotoolkit.data.FeatureWriter;
import org.geotoolkit.data.jdbc.FilterToSQL;
import org.geotoolkit.data.jdbc.FilterToSQLException;
import org.geotoolkit.data.jdbc.datasource.ManageableDataSource;
import org.geotoolkit.data.jdbc.fidmapper.FIDMapper;
import org.geotoolkit.data.memory.GenericFilterFeatureIterator;
import org.geotoolkit.data.memory.GenericReprojectFeatureIterator;
import org.geotoolkit.data.memory.GenericRetypeFeatureIterator;
import org.geotoolkit.data.query.DefaultQueryCapabilities;
import org.geotoolkit.data.query.Query;
import org.geotoolkit.data.query.QueryBuilder;
import org.geotoolkit.data.query.QueryCapabilities;
import org.geotoolkit.factory.Hints;
import org.geotoolkit.factory.HintsPending;
import org.geotoolkit.feature.AttributeDescriptorBuilder;
import org.geotoolkit.feature.AttributeTypeBuilder;
import org.geotoolkit.feature.DefaultName;
import org.geotoolkit.feature.simple.SimpleFeatureBuilder;
import org.geotoolkit.feature.simple.SimpleFeatureTypeBuilder;
import org.geotoolkit.filter.capability.DefaultFilterCapabilities;
import org.geotoolkit.filter.visitor.CapabilitiesFilterSplitter;
import org.geotoolkit.filter.visitor.FilterAttributeExtractor;
import org.geotoolkit.filter.visitor.SimplifyingFilterVisitor;
import org.geotoolkit.geometry.jts.JTSEnvelope2D;
import org.geotoolkit.jdbc.fid.AutoGeneratedPrimaryKeyColumn;
import org.geotoolkit.jdbc.fid.NonIncrementingPrimaryKeyColumn;
import org.geotoolkit.jdbc.fid.SequencedPrimaryKeyColumn;
import org.geotoolkit.referencing.CRS;
import org.geotoolkit.util.Converters;

import org.opengis.feature.Feature;
import org.opengis.feature.simple.SimpleFeature;
import org.opengis.feature.simple.SimpleFeatureType;
import org.opengis.feature.type.AttributeDescriptor;
import org.opengis.feature.type.FeatureType;
import org.opengis.feature.type.GeometryDescriptor;
import org.opengis.feature.type.Name;
import org.opengis.feature.type.PropertyDescriptor;
import org.opengis.filter.Filter;
import org.opengis.filter.PropertyIsLessThanOrEqualTo;
import org.opengis.filter.expression.Function;
import org.opengis.filter.expression.Literal;
import org.opengis.filter.expression.PropertyName;
import org.opengis.filter.identity.FeatureId;
import org.opengis.filter.sort.SortBy;
import org.opengis.filter.sort.SortOrder;
import org.opengis.geometry.Envelope;
import org.opengis.referencing.FactoryException;
import org.opengis.referencing.crs.CoordinateReferenceSystem;

import static org.geotoolkit.jdbc.MetaDataConstants.*;

/**
 * @author Johann Sorel (Geomatys)
 *
 * @module pending
 */
public final class DefaultJDBCDataStore extends AbstractJDBCDataStore {

    /**
     * writer flags
     */
    protected final int WRITER_ADD = 0x01<<0;
    protected final int WRITER_UPDATE = 0x01<<1;

    private final QueryCapabilities capabilities = new DefaultQueryCapabilities(false);
    private final Map<Name,FeatureType> names = new HashMap<Name, FeatureType>();
    private final Map<Name,PrimaryKey> primaryKeys = new HashMap<Name, PrimaryKey>();
    private Set<Name> nameCache = null;

    DefaultJDBCDataStore(String namespace){
        super(namespace);
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public PrimaryKey getPrimaryKey(FeatureType type) throws DataStoreException{
        if(nameCache == null){
            visitTables();
        }
        return primaryKeys.get(type.getName());
    }

    @Override
    public boolean isWritable(Name typeName) throws DataStoreException {
        final PrimaryKey key = getPrimaryKey(getFeatureType(typeName));
        return key != null && !(key instanceof NullPrimaryKey);
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public synchronized Set<Name> getNames() throws DataStoreException {
        Set<Name> ref = nameCache;
        if(ref == null){
            visitTables();
            ref = Collections.unmodifiableSet(new HashSet<Name>(names.keySet()));
            nameCache = ref;
        }
        return ref;
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public FeatureType getFeatureType(Name typeName) throws DataStoreException {
        typeCheck(typeName);
        return names.get(typeName);
    }

    /**
     * Explore the available tables and generate schemas and primary keys.
     * @throws DataStoreException
     */
    private synchronized void visitTables() throws DataStoreException{

        //clear previous schemas, this might be called after an update schema
        nameCache = null;
        names.clear();
        primaryKeys.clear();

        /*
         *        <LI><B>TABLE_CAT</B> String => table catalog (may be <code>null</code>)
         *        <LI><B>TABLE_SCHEM</B> String => table schema (may be <code>null</code>)
         *        <LI><B>TABLE_NAME</B> String => table name
         *        <LI><B>TABLE_TYPE</B> String => table type.  Typical types are "TABLE",
         *                        "VIEW",        "SYSTEM TABLE", "GLOBAL TEMPORARY",
         *                        "LOCAL TEMPORARY", "ALIAS", "SYNONYM".
         *        <LI><B>REMARKS</B> String => explanatory comment on the table
         *  <LI><B>TYPE_CAT</B> String => the types catalog (may be <code>null</code>)
         *  <LI><B>TYPE_SCHEM</B> String => the types schema (may be <code>null</code>)
         *  <LI><B>TYPE_NAME</B> String => type name (may be <code>null</code>)
         *  <LI><B>SELF_REFERENCING_COL_NAME</B> String => name of the designated
         *                  "identifier" column of a typed table (may be <code>null</code>)
         *        <LI><B>REF_GENERATION</B> String => specifies how values in
         *                  SELF_REFERENCING_COL_NAME are created. Values are
         *                  "SYSTEM", "USER", "DERIVED". (may be <code>null</code>)
         */
        Connection cx = null;
        try {
            cx = createConnection();
            final DatabaseMetaData metaData = cx.getMetaData();
            final ResultSet tables = metaData.getTables(null, databaseSchema, "%", new String[]{MD_TABLE, MD_VIEW});

            try {
                while (tables.next()) {
                    final String schemaName = tables.getString(MD_TABLE_SCHEM);
                    final String tableName = tables.getString(MD_TABLE_NAME);

                    //use the dialect to filter
                    if (!dialect.includeTable(schemaName, tableName, cx)) {
                        continue;
                    }

                    final Name name = new DefaultName(getNamespaceURI(), tableName);
                    final SimpleFeatureType sft = buildFeatureType(name);
                    final PrimaryKey pkey = getPrimaryKey(name);
                    names.put(name, sft);
                    primaryKeys.put(name, pkey);
                }
            } finally {
                closeSafe(tables);
            }
        } catch (SQLException e) {
            throw (DataStoreException) new DataStoreException("Error occurred getting table name list.").initCause(e);
        } catch (IOException e) {
            throw (DataStoreException) new DataStoreException("Error occurred getting table name list.").initCause(e);
        } finally {
            closeSafe(cx);
        }

    }

    /**
     * Builds the feature type from database metadata.
     */
    private SimpleFeatureType buildFeatureType(Name typeName) throws IOException {
        final SimpleFeatureTypeBuilder tb = new SimpleFeatureTypeBuilder();
        final AttributeDescriptorBuilder adb = new AttributeDescriptorBuilder();
        final AttributeTypeBuilder atb = new AttributeTypeBuilder();

        //set up the name
        final String tableName = typeName.getLocalPart();
        final String namespace = typeName.getNamespaceURI();
        tb.setName(typeName);

        //ensure we have a connection
        Connection cx = null;
        try {
            cx = createConnection();
            //get metadata about columns from database
            final DatabaseMetaData metaData = cx.getMetaData();

            /*
             *        <LI><B>COLUMN_NAME</B> String => column name
             *        <LI><B>DATA_TYPE</B> int => SQL type from java.sql.Types
             *        <LI><B>TYPE_NAME</B> String => Data source dependent type name,
             *  for a UDT the type name is fully qualified
             *        <LI><B>COLUMN_SIZE</B> int => column size.  For char or date
             *            types this is the maximum number of characters, for numeric or
             *            decimal types this is precision.
             *        <LI><B>BUFFER_LENGTH</B> is not used.
             *        <LI><B>DECIMAL_DIGITS</B> int => the number of fractional digits
             *        <LI><B>NUM_PREC_RADIX</B> int => Radix (typically either 10 or 2)
             *        <LI><B>NULLABLE</B> int => is NULL allowed.
             *      <UL>
             *      <LI> columnNoNulls - might not allow <code>NULL</code> values
             *      <LI> columnNullable - definitely allows <code>NULL</code> values
             *      <LI> columnNullableUnknown - nullability unknown
             *      </UL>
             *         <LI><B>COLUMN_DEF</B> String => default value (may be <code>null</code>)
             *        <LI><B>IS_NULLABLE</B> String => "NO" means column definitely
             *      does not allow NULL values; "YES" means the column might
             *      allow NULL values.  An empty string means nobody knows.
             */
            final ResultSet columns = metaData.getColumns(null, databaseSchema, tableName, "%");

            try {
                columnIte :
                while (columns.next()) {
                    adb.reset();
                    atb.reset();

                    final String name = columns.getString(MD_COLUMN_NAME);

                    //we need the primary keys as fields since join query rely on them.
//                    //encomment to not include primary key in the type
//                    /*
//                     *        <LI><B>TABLE_CAT</B> String => table catalog (may be <code>null</code>)
//                     *        <LI><B>TABLE_SCHEM</B> String => table schema (may be <code>null</code>)
//                     *        <LI><B>TABLE_NAME</B> String => table name
//                     *        <LI><B>COLUMN_NAME</B> String => column name
//                     *        <LI><B>KEY_SEQ</B> short => sequence number within primary key
//                     *        <LI><B>PK_NAME</B> String => primary key name (may be <code>null</code>)
//                     */
//                    final ResultSet primaryKeys = metaData.getPrimaryKeys(null, databaseSchema, tableName);
//                    try {
//                        while (primaryKeys.next()) {
//                            if (name.equals(primaryKeys.getString(MD_COLUMN_NAME))) {
//                                continue columnIte;
//                            }
//                        }
//                    } finally {
//                        closeSafe(primaryKeys);
//                    }

                    //figure out the type mapping

                    //first ask the dialect
                    Class binding = dialect.getMapping(columns, cx);

                    if (binding == null) {
                        //determine from type mappings
                        final int dataType = columns.getInt(MD_DATA_TYPE);
                        binding = dialect.getMapping(dataType);
                    }

                    if (binding == null) {
                        //determine from type name mappings
                        final String tn = columns.getString(MD_TYPE_NAME);
                        binding = dialect.getMapping(tn);
                    }

                    //if still not found, resort to Object
                    if (binding == null) {
                        getLogger().warning("Could not find mapping for:" + name);
                        binding = Object.class;
                    }

                    //nullability
                    if ( MD_NOT_NULL.equalsIgnoreCase( columns.getString(MD_IS_NULLABLE) ) ) {
                        adb.setNillable(false);
                        adb.setMinOccurs(0);
                    }else{
                        adb.setNillable(true);
                        adb.setMinOccurs(0);
                    }

                    //primary key never null, min one
                    final ResultSet primaryKeys = metaData.getPrimaryKeys(null, databaseSchema, tableName);
                    try {
                        while (primaryKeys.next()) {
                            if (name.equals(primaryKeys.getString(MD_COLUMN_NAME))) {
                                adb.setNillable(false);
                                adb.setMinOccurs(1);
                                break;
                            }
                        }
                    } finally {
                        closeSafe(primaryKeys);
                    }


                    //determine if this attribute is a geometry or not
                    if (Geometry.class.isAssignableFrom(binding)) {
                        //add the attribute as a geometry, try to figure out
                        // its srid first
                        Integer srid = null;
                        CoordinateReferenceSystem crs = null;
                        try {
                            srid = dialect.getGeometrySRID(databaseSchema, tableName, name, cx);
                            if(srid != null)
                                crs = dialect.createCRS(srid, cx);
                        } catch (SQLException e) {
                            String msg = "Error occured determing srid for " + tableName + "."+ name;
                            getLogger().log(Level.WARNING, msg, e);
                        }

                        atb.setBinding(binding);
                        atb.setName(ensureGMLNS(namespace,name));
                        atb.setCRS(crs);
                        if(srid != null) adb.addUserData(JDBCDataStore.JDBC_NATIVE_SRID, srid);
                        adb.setName(ensureGMLNS(namespace,name));
                        adb.setType(atb.buildGeometryType());
                        tb.add(adb.buildDescriptor());
                    } else {
                        //add the attribute
                        Name attName = ensureGMLNS(namespace, name);
                        atb.setName(attName);
                        atb.setBinding(binding);
                        adb.setName(attName);
                        adb.setType(atb.buildType());
                        tb.add(adb.buildDescriptor());
                    }
                }

                return tb.buildFeatureType();
            } finally {
                closeSafe(columns);
            }
        } catch (SQLException e) {
            String msg = "Error occurred building feature type";
            throw (IOException) new IOException(msg).initCause(e);
        }finally {
            closeSafe(cx);
        }
    }

    /**
     * Returns the primary key object for a particular entry, deriving it from
     * the underlying database metadata.
     */
    protected PrimaryKey getPrimaryKey(final Name entry) throws IOException {

        PrimaryKey pkey;

        Connection cx = null;
        try {
            cx = createConnection();
            final String tableName = entry.getLocalPart();
            //get metadata from database
            final DatabaseMetaData metaData = cx.getMetaData();
            final ResultSet primaryKey = metaData.getPrimaryKeys(null, databaseSchema, tableName);

            try {
                /*
                 *        <LI><B>TABLE_CAT</B> String => table catalog (may be <code>null</code>)
                 *        <LI><B>TABLE_SCHEM</B> String => table schema (may be <code>null</code>)
                 *        <LI><B>TABLE_NAME</B> String => table name
                 *        <LI><B>COLUMN_NAME</B> String => column name
                 *        <LI><B>KEY_SEQ</B> short => sequence number within primary key
                 *        <LI><B>PK_NAME</B> String => primary key name (may be <code>null</code>)
                 */
                final ArrayList<PrimaryKeyColumn> cols = new ArrayList();

                while (primaryKey.next()) {
                    final String columnName = primaryKey.getString(MD_COLUMN_NAME);

                    //look up the type ( should only be one row )
                    final ResultSet columns = metaData.getColumns(null, databaseSchema, tableName, columnName);
                    columns.next();

                    final int binding = columns.getInt(MD_DATA_TYPE);
                    Class columnType = dialect.getMapping(binding);

                    if (columnType == null) {
                        getLogger().warning("No class for sql type " + binding);
                        columnType = Object.class;
                    }

                    //determine which type of primary key we have
                    PrimaryKeyColumn col = null;

                    //1. Auto Incrementing?
                    final Statement st = cx.createStatement();

                    try {
                        //not actually going to get data
                        st.setFetchSize(1);

                        final StringBuilder sql = new StringBuilder();
                        sql.append("SELECT ");
                        dialect.encodeColumnName(columnName, sql);
                        sql.append(" FROM ");
                        encodeTableName(tableName, sql);

                        sql.append(" WHERE 0=1");

                        getLogger().log(Level.FINE, "Grabbing table pk metadata: {0}", sql);

                        ResultSet rs = null;
                        try {
                            rs = st.executeQuery(sql.toString());

                            if (rs.getMetaData().isAutoIncrement(1)) {
                                col = new AutoGeneratedPrimaryKeyColumn(columnName, columnType);
                            }
                        } finally {
                            closeSafe(rs);
                        }
                    } finally {
                        closeSafe(st);
                    }

                    //2. Has a sequence?
                    if (col == null) {
                        //TODO: look for a sequence
                        final String sequenceName = dialect.getSequenceForColumn(databaseSchema,
                                tableName, columnName, cx);
                        if (sequenceName != null) {
                            col = new SequencedPrimaryKeyColumn(columnName, columnType, sequenceName);
                        }
                    }

                    if (col == null) {
                        col = new NonIncrementingPrimaryKeyColumn(columnName, columnType);
                    }

                    cols.add(col);
                }

                if (cols.isEmpty()) {
                    getLogger().info("No primary key found for " + tableName + ".");
                    pkey = new NullPrimaryKey(tableName);
                } else {
                    pkey = new PrimaryKey(tableName, cols);
                }

            } finally {
                closeSafe(primaryKey);
            }
        } catch (SQLException e) {
            throw (IOException) new IOException("Error looking up primary key").initCause(e);
        } finally {
            closeSafe(cx);
        }

        return pkey;
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public QueryCapabilities getQueryCapabilities() {
        return capabilities;
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public List<FeatureId> addFeatures(Name groupName, Collection<? extends Feature> newFeatures) throws DataStoreException {
        return handleAddWithFeatureWriter(groupName, newFeatures);
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public void updateFeatures(Name groupName, Filter filter, Map<? extends PropertyDescriptor, ? extends Object> values) throws DataStoreException {
        handleUpdateWithFeatureWriter(groupName, filter, values);
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public void removeFeatures(Name groupName, Filter filter) throws DataStoreException {
        handleRemoveWithFeatureWriter(groupName, filter);
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public FeatureReader getFeatureReader(Query query) throws DataStoreException {
        typeCheck(query.getTypeName());
        final SimpleFeatureType type = (SimpleFeatureType) getFeatureType(query.getTypeName());

        // split the filter
        final Filter[] split = splitFilter(query.getFilter(),type);
        final Filter preFilter = split[0];
        final Filter postFilter = split[1];

        // rebuild a new query with the same params, but just the pre-filter
        final QueryBuilder builder = new QueryBuilder(query);
        builder.setFilter(preFilter);
        final Query preQuery = builder.buildQuery();

        // Build the feature type returned by this query. Also build an eventual extra feature type
        // containing the attributes we might need in order to evaluate the post filter
        SimpleFeatureType querySchema;
        SimpleFeatureType returnedSchema;
        if(query.retrieveAllProperties()) {
            returnedSchema = querySchema = type;
        } else {
            returnedSchema = SimpleFeatureTypeBuilder.retype(type, query.getPropertyNames());
            FilterAttributeExtractor extractor = new FilterAttributeExtractor(type);
            postFilter.accept(extractor, null);
            Name[] extraAttributes = extractor.getAttributeNames();
            if(extraAttributes == null || extraAttributes.length == 0) {
                querySchema = returnedSchema;
            } else {
                List<Name> allAttributes = new ArrayList<Name>(Arrays.asList(query.getPropertyNames()));
                for (Name extraAttribute : extraAttributes) {
                    if(!allAttributes.contains(extraAttribute)) {
                        allAttributes.add(extraAttribute);
                    }
                }
                Name[] allAttributeArray = allAttributes.toArray(new Name[allAttributes.size()]);
                querySchema = SimpleFeatureTypeBuilder.retype(type, allAttributeArray);
            }
        }

        //grab connection
        final Connection cx;
        try {
            cx = createConnection();
        } catch (SQLException ex) {
            throw new DataStoreException(ex);
        }

        //create the reader
        FeatureReader<SimpleFeatureType, SimpleFeature> reader;

        try {
            // this allows PostGIS to page the results and respect the fetch size
            // if (getState().getTransaction() == Transaction.AUTO_COMMIT)
            cx.setAutoCommit(false);

            final SQLDialect dialect = getDialect();
            if (dialect instanceof PreparedStatementSQLDialect) {
                final PreparedStatement ps = selectSQLPS(querySchema, preQuery, cx);
                reader = new JDBCFeatureReader(ps, cx, this, query.getTypeName(), querySchema, query.getHints());
            } else {
                //build up a statement for the content
                final String sql = selectSQL(querySchema, preQuery);
                getLogger().fine(sql);

                reader = new JDBCFeatureReader( sql, cx, this, query.getTypeName(), querySchema, query.getHints() );
            }
        } catch (SQLException e) {
            // close the connection
            closeSafe(cx);
            // safely rethrow
            throw (DataStoreException) new DataStoreException().initCause(e);
        } catch (IOException e) {
            // close the connection
            closeSafe(cx);
            // safely rethrow
            throw (DataStoreException) new DataStoreException().initCause(e);
        }


        // if post filter, wrap it
        if (postFilter != null && postFilter != Filter.INCLUDE) {
            reader = GenericFilterFeatureIterator.wrap(reader, postFilter);
            if(!returnedSchema.equals(querySchema))
                reader = GenericRetypeFeatureIterator.wrap(reader, returnedSchema,query.getHints());
        }

        final CoordinateReferenceSystem reproject = query.getCoordinateSystemReproject();
        if(reproject != null && !CRS.equalsIgnoreMetadata(reproject,type.getCoordinateReferenceSystem())){
            try {
                reader = GenericReprojectFeatureIterator.wrap(reader, reproject);
            } catch (FactoryException ex) {
                throw new DataStoreException(ex);
            } catch (SchemaException ex) {
                throw new DataStoreException(ex);
            }
        }

        return reader;
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public FeatureWriter getFeatureWriter(Name typeName, Filter filter) throws DataStoreException {
        try {
            return getFeatureWriterInternal(typeName, filter, WRITER_ADD | WRITER_UPDATE);
        } catch (IOException ex) {
            throw new DataStoreException(ex);
        }
    }

    @Override
    public FeatureWriter getFeatureWriterAppend(Name typeName) throws DataStoreException {
        try {
            return getFeatureWriterInternal(typeName, Filter.EXCLUDE, WRITER_ADD);
        } catch (IOException ex) {
            throw new DataStoreException(ex);
        }
    }

    private FeatureWriter getFeatureWriterInternal(final Name typeName, final Filter filter, final int flags)
            throws DataStoreException, IOException {

        if(!isWritable(typeName)){
            throw new DataStoreException("Type "+ typeName + " is not writeable.");
        }
        if (flags == 0) {
            throw new IllegalArgumentException( "no write flags set" );
        }

        final SimpleFeatureType type = (SimpleFeatureType) getFeatureType(typeName);

        //split the filter
        final Filter[] split = splitFilter(filter,type);


        Connection cx = null;
        FeatureWriter<SimpleFeatureType, SimpleFeature> writer;
        try {
            cx = createConnection();
            //check for insert only
            if ( (flags | WRITER_ADD) == WRITER_ADD ) {
                //build up a statement for the content, inserting only so we dont want
                //the query to return any data ==> Filter.EXCLUDE
                final Query queryNone = QueryBuilder.filtered(typeName, Filter.EXCLUDE);
                if ( getDialect() instanceof PreparedStatementSQLDialect ) {
                    final PreparedStatement ps = selectSQLPS(type, queryNone, cx);
                    return new JDBCInsertFeatureWriter( ps, cx, this, typeName, type, null );
                }else{
                    final String sql = selectSQL(type, queryNone);
                    getLogger().fine(sql);
                    return new JDBCInsertFeatureWriter( sql, cx, this, typeName, type, null );
                }
            }


            // build up a statement for the content
            final Query preQuery = QueryBuilder.filtered(typeName, split[0]);

            if(getDialect() instanceof PreparedStatementSQLDialect) {
                final PreparedStatement ps = selectSQLPS(type, preQuery, cx);
                if ( (flags | WRITER_UPDATE) == WRITER_UPDATE ) {
                    writer = new JDBCUpdateFeatureWriter(ps, cx, this, typeName, type, null );
                } else {
                    //update insert case
                    writer = new JDBCUpdateInsertFeatureWriter(ps, cx, this, typeName, type, null, null );
                }
            } else {
                final String sql = selectSQL(type, preQuery);
                getLogger().fine(sql);

                if ( (flags | WRITER_UPDATE) == WRITER_UPDATE ) {
                    writer = new JDBCUpdateFeatureWriter( sql, cx, this, typeName, type, null );
                } else {
                    //update insert case
                    writer = new JDBCUpdateInsertFeatureWriter( sql, cx, this, typeName, type, null );
                }
            }

        } catch (SQLException e) {
            // close the connection
            closeSafe(cx);
            // now we can safely rethrow the exception
            throw (DataStoreException) new DataStoreException().initCause(e);
        }

        //check for post filter and wrap accordingly
        if ( split[1] != null && split[1] != Filter.INCLUDE ) {
            writer = GenericFilterFeatureIterator.wrap(writer, split[1]);
        }
        return writer;
    }


    /**
     * {@inheritDoc }
     */
    @Override
    public long getCount(Query query) throws DataStoreException {
        typeCheck(query.getTypeName());
        final SimpleFeatureType type = (SimpleFeatureType) getFeatureType(query.getTypeName());

        //split the filter
        final Filter[] split = splitFilter( query.getFilter(),type );
        final Filter preFilter = split[0];
        final Filter postFilter = split[1];


        if ((postFilter != null) && (postFilter != Filter.INCLUDE)) {
            try {
                //calculate manually, dont use datastore optimization
                getLogger().fine("Calculating size manually");

                int count = 0;

                //grab a reader
                FeatureReader<SimpleFeatureType, SimpleFeature> reader = getFeatureReader(query);
                try {
                    while (reader.hasNext()) {
                        reader.next();
                        count++;
                    }
                } finally {
                    reader.close();
                }

                return count;
            } catch (Exception e) {
                throw new RuntimeException(e);
            }
        } else {
            //no post filter, we have a preFilter, or preFilter is null..
            // either way we can use the datastore optimization
            final Connection cx;
            try {
                cx = createConnection();
            } catch (SQLException ex) {
                throw new DataStoreException(ex);
            }
            try {
                final QueryBuilder builder = new QueryBuilder(query);
                builder.setFilter(preFilter);
                final Query q = builder.buildQuery();
                int count = getCount(type, q, cx);
                // if native support for limit and offset is not implemented, we have to ajust the result
                if (!getDialect().isLimitOffsetSupported()) {
                    if (query.getStartIndex() > 0) {
                        if (query.getStartIndex() > count) {
                            count = 0;
                        } else {
                            count -= query.getStartIndex();
                        }
                    }
                    if (query.getMaxFeatures() > 0 && count > query.getMaxFeatures()) {
                        count = query.getMaxFeatures();
                    }
                }
                return count;
            } finally {
                closeSafe(cx);
            }
        }
    }

    /**
     * Returns the count of the features for a particular feature type / table.
     * Rely on the database count capabilities.
     */
    private int getCount(final SimpleFeatureType featureType, final Query query, final Connection cx)
            throws DataStoreException {

        final Statement st;
        final ResultSet rs;
        try {
            if (dialect instanceof PreparedStatementSQLDialect) {
                st = selectCountSQLPS(featureType, query, cx);
                rs = ((PreparedStatement) st).executeQuery();
            } else {
                final String sql = selectCountSQL(featureType, query);
                getLogger().log(Level.FINE, "Counting features: {0}", sql);

                st = cx.createStatement();
                rs = st.executeQuery(sql);
            }

            try {
                rs.next();
                return rs.getInt(1);
            } finally {
                closeSafe(rs);
                closeSafe(st);
            }
        } catch (SQLException e) {
            throw (DataStoreException) new DataStoreException("Error occured calculating count").initCause(e);
        }
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public Envelope getEnvelope(Query query) throws DataStoreException, DataStoreRuntimeException {
        typeCheck(query.getTypeName());
        final SimpleFeatureType type = (SimpleFeatureType) getFeatureType(query.getTypeName());

        //split the filter
        final Filter[] split = splitFilter(query.getFilter(),type);
        final Filter preFilter = split[0];
        final Filter postFilter = split[1];

        final boolean canLimitOffset = dialect.isLimitOffsetSupported();

        if ((postFilter != null) && (postFilter != Filter.INCLUDE) || (query.getMaxFeatures() != null && !canLimitOffset)
                                     || (query.getStartIndex() > 0 && !canLimitOffset)) {
            //calculate manually, don't use datastore optimization
            getLogger().fine("Calculating bounds manually");

            // grab the 2d part of the crs
            final CoordinateReferenceSystem flatCRS = CRS.getHorizontalCRS(type.getCoordinateReferenceSystem());
            final JTSEnvelope2D bounds = new JTSEnvelope2D(flatCRS);

            // grab a reader
            final QueryBuilder builder = new QueryBuilder(query);
            builder.setFilter(postFilter);
            final Query q = builder.buildQuery();
            final FeatureReader<SimpleFeatureType, SimpleFeature> i = getFeatureReader(q);
            try {
                if (i.hasNext()) {
                    SimpleFeature f = (SimpleFeature) i.next();
                    bounds.init(f.getBounds());

                    while (i.hasNext()) {
                        f = i.next();
                        bounds.include(f.getBounds());
                    }
                }
            } finally {
                i.close();
            }

            return bounds;
        } else {
            //post filter was null... pre can be set or null... either way
            // use datastore optimization
            final Connection cx;
            try {
                cx = createConnection();
            } catch (SQLException ex) {
                throw new DataStoreException(ex);
            }
            try {
                final QueryBuilder builder = new QueryBuilder(query);
                builder.setFilter(preFilter);
                final Query q = builder.buildQuery();
                return getEnvelope(type, q, cx);
            } finally {
                closeSafe(cx);
            }
        }
    }

    /**
     * Returns the bounds of the features for a particular feature type / table.
     * Rely on tha database to obtain envelope.
     *
     * @param featureType The feature type / table.
     * @param query Specifies rows to include in bounds calculation, as well as how many
     *              features and the offset if needed
     */
    private Envelope getEnvelope(final SimpleFeatureType featureType, final Query query,
            final Connection cx) throws DataStoreException {

        // handle geometryless case by returning an empty envelope
        if (featureType.getGeometryDescriptor() == null) {
            return EMPTY_ENVELOPE;
        }

        final Statement st;
        final ResultSet rs;
        try {
            if (dialect instanceof PreparedStatementSQLDialect) {
                st = selectBoundsSQLPS(featureType, query, cx);
                rs = ((PreparedStatement) st).executeQuery();
            } else {
                final String sql = selectBoundsSQL(featureType, query);
                getLogger().log(Level.FINE, "Retriving bounding box: {0}", sql);

                st = cx.createStatement();
                rs = st.executeQuery(sql);
            }

            try {
                final JTSEnvelope2D bounds;
                com.vividsolutions.jts.geom.Envelope e = null;
                if (rs.next()) {
                    try {
                        e = dialect.decodeGeometryEnvelope(rs, 1, st.getConnection());
                    } catch (IOException ex) {
                        throw new DataStoreException(ex);
                    }
                }

                if (e == null) {
                    e = new com.vividsolutions.jts.geom.Envelope();
                    e.init(0, 0, 0, 0);
                    //e.setToNull();
                }

                if (e instanceof JTSEnvelope2D) {
                    bounds = (JTSEnvelope2D) e;
                } else {
                    //set the crs to be the crs of the feature type
                    // grab the 2d part of the crs
                    final CoordinateReferenceSystem flatCRS = CRS.getHorizontalCRS(
                            featureType.getCoordinateReferenceSystem());

                    if (e != null) {
                        bounds = new JTSEnvelope2D(e, flatCRS);
                    } else {
                        bounds = new JTSEnvelope2D(flatCRS);
                        bounds.setToNull();
                    }
                }

                //keep going to handle case where envelope is not calculated
                // as aggregate function
                if (e.isNull() == false) { // featuretype not empty
                    while (rs.next()) {
                        try {
                            bounds.expandToInclude(dialect.decodeGeometryEnvelope(rs, 1, st.getConnection()));
                        } catch (IOException ex) {
                            throw new DataStoreException(ex);
                        }
                    }
                }

                return bounds;
            } finally {
                closeSafe(rs);
                closeSafe(st);
            }
        } catch (SQLException e) {
            throw (DataStoreException) new DataStoreException("Error occured calculating bounds").initCause(e);
        }
    }

    /**
     * Inserts a new feature into the database for a particular feature type / table.
     */
    protected void insert(final SimpleFeature feature, final SimpleFeatureType featureType,
            final Connection cx) throws DataStoreException {
        insert(Collections.singletonList(feature), featureType, cx);
    }

    /**
     * Inserts a collection of new features into the database for a particular
     * feature type / table.
     */
    protected void insert(final Collection features, final SimpleFeatureType featureType,
            final Connection cx) throws DataStoreException {
        final PrimaryKey key = getPrimaryKey(featureType);

        // we do this in a synchronized block because we need to do two queries,
        // first to figure out what the id will be, then the insert statement
        synchronized (this) {
            Statement st = null;
            try {
                if (!(dialect instanceof PreparedStatementSQLDialect)) {
                    st = cx.createStatement();
                }

                //figure out what the next fid will be
                final List<Object> nextKeyValues = getNextValues(key, cx);

                for (Iterator f = features.iterator(); f.hasNext();) {
                    final SimpleFeature feature = (SimpleFeature) f.next();

                    if (dialect instanceof PreparedStatementSQLDialect) {
                        final PreparedStatement ps = insertSQLPS(featureType, feature, nextKeyValues, cx);
                        try {
                            ps.execute();
                        } finally {
                            closeSafe(ps);
                        }
                    } else {
                        String sql = insertSQL(featureType, feature, nextKeyValues, cx);
                        getLogger().log(Level.FINE, "Inserting new feature: {0}", sql);

                        //TODO: execute in batch to improve performance?
                        st.execute(sql);
                    }

                    //report the feature id as user data since we cant set the fid
                    String fid = featureType.getTypeName() + "." + encodeFID(nextKeyValues);
                    feature.getUserData().put("fid", fid);
                }

            //st.executeBatch();
            } catch (SQLException e) {
                throw (DataStoreException) new DataStoreException("Error inserting features").initCause(e);
            } catch (IOException e) {
                throw (DataStoreException) new DataStoreException("Error inserting features").initCause(e);
            }finally {
                closeSafe(st);
            }
        }
    }

    /**
     * Updates an existing feature(s) in the database for a particular feature type / table.
     */
    protected void update(final SimpleFeatureType featureType, final List<AttributeDescriptor> attributes,
            final List<Object> values, final Filter filter, final Connection cx) throws DataStoreException{
        update(featureType, attributes.toArray(new AttributeDescriptor[attributes.size()]),
                values.toArray(new Object[values.size()]), filter, cx);
    }

    /**
     * Updates an existing feature(s) in the database for a particular feature type / table.
     */
    protected void update(final SimpleFeatureType featureType, final AttributeDescriptor[] attributes,
            final Object[] values, final Filter filter, final Connection cx) throws DataStoreException{
        if ((attributes == null) || (attributes.length == 0)) {
            getLogger().warning("Update called with no attributes, doing nothing.");

            return;
        }

        if (dialect instanceof PreparedStatementSQLDialect) {
            try {
                final PreparedStatement ps = updateSQLPS(featureType, attributes, values, filter, cx);
                try {
                    ps.execute();
                }
                finally {
                    closeSafe( ps );
                }
            }catch (SQLException e) {
                throw (DataStoreException) new DataStoreException("Error occured updating features").initCause(e);
            }catch (IOException e) {
                throw (DataStoreException) new DataStoreException("Error occured updating features").initCause(e);
            }
        } else {
            try {
                final String sql = updateSQL(featureType, attributes, values, filter);
                getLogger().log(Level.FINE, "Updating feature: {0}", sql);

                final Statement st = cx.createStatement();

                try {
                    st.execute(sql);
                } finally {
                    closeSafe(st);
                }
            } catch (SQLException e) {
                throw (DataStoreException) new DataStoreException("Error occured updating features").initCause(e);
            } catch (IOException e) {
                throw (DataStoreException) new DataStoreException("Error occured updating features").initCause(e);
            }
        }
    }

    /**
     * Deletes an existing feature in the database for a particular feature type / fid.
     */
    protected void delete(final SimpleFeatureType featureType, final String fid,
            final Connection cx) throws IOException{
        final Filter filter = filterFactory.id(Collections.singleton(filterFactory.featureId(fid)));
        delete(featureType, filter, cx);
    }

    /**
     * Deletes an existing feature(s) in the database for a particular feature type / table.
     */
    protected void delete(final SimpleFeatureType featureType, final Filter filter, final Connection cx)
            throws IOException {

        Statement st = null;
        try {
            if (dialect instanceof PreparedStatementSQLDialect) {
                st = deleteSQLPS(featureType, filter, cx);
                ((PreparedStatement) st).execute();
            } else {
                final String sql = deleteSQL(featureType, filter);
                getLogger().log(Level.FINE, "Removing feature(s): {0}", sql);

                st = cx.createStatement();
                st.execute(sql);
            }
        } catch (SQLException e) {
            throw (IOException) new IOException("Error occured calculating bounds").initCause(e);
        } finally {
            closeSafe(st);
        }
    }


    ////////////////////////////////////////////////////////////////////////////
    // schema manipulation /////////////////////////////////////////////////////
    ////////////////////////////////////////////////////////////////////////////

    /**
     * {@inheritDoc }
     */
    @Override
    public void createSchema(Name typeName, FeatureType featureType) throws DataStoreException {

        if(typeName == null){
            throw new DataStoreException("Type name can not be null.");
        }

        if(!(featureType instanceof SimpleFeatureType)){
            throw new DataStoreException("JDBC datastore can handle only simple feature types.");
        }

        if(!featureType.getName().equals(typeName)){
            throw new DataStoreException("JDBC datastore can only hold typename same as feature type name.");
        }

        if(getNames().contains(typeName)){
            throw new DataStoreException("Type name "+ typeName + " already exists.");
        }


        //execute the create table statement
        //TODO: create a primary key and a spatial index
        Connection cx = null;

        try {
            cx = createConnection();
            final String sql = createTableSQL((SimpleFeatureType) featureType,cx);
            getLogger().log(Level.FINE, "Create schema: {0}", sql);

            final Statement st = cx.createStatement();

            try {
                st.execute(sql);
            } finally {
                closeSafe(st);
            }

            dialect.postCreateTable(databaseSchema, (SimpleFeatureType)featureType, cx);
        } catch (Exception e) {
            throw (DataStoreException) new DataStoreException("Error occurred creating table").initCause(e);
        } finally {
            closeSafe(cx);
        }

        // reset the type name cache, will be recreated when needed.
        nameCache = null;
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public void updateSchema(Name typeName, FeatureType featureType) throws DataStoreException {
        throw new UnsupportedOperationException("Not supported yet.");
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public void deleteSchema(Name typeName) throws DataStoreException {
        throw new UnsupportedOperationException("Not supported yet.");
    }

    ////////////////////////////////////////////////////////////////////////////
    // Connection utils ////////////////////////////////////////////////////////
    ////////////////////////////////////////////////////////////////////////////

    /**
     * {@inheritDoc }
     */
    @Override
    public final Connection createConnection() throws SQLException {
        getLogger().fine("CREATE CONNECTION");
        final Connection cx = getDataSource().getConnection();
        // isolation level is not set in the datastore, see
        // http://jira.codehaus.org/browse/GEOT-2021

        //call dialect callback to iniitalie the connection
        dialect.initializeConnection(cx);
        return cx;
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public void closeSafe(final ResultSet rs) {
        if (rs == null) {
            return;
        }

        try {
            rs.close();
        } catch (SQLException e) {
            String msg = "Error occurred closing result set";
            getLogger().warning(msg);

            if (getLogger().isLoggable(Level.FINER)) {
                getLogger().log(Level.FINER, msg, e);
            }
        }
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public void closeSafe(final Statement st) {
        if (st == null) {
            return;
        }

        try {
            st.close();
        } catch (SQLException e) {
            String msg = "Error occurred closing statement";
            getLogger().warning(msg);

            if (getLogger().isLoggable(Level.FINER)) {
                getLogger().log(Level.FINER, msg, e);
            }
        }
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public void closeSafe(final Connection cx) {
        if (cx == null) {
            return;
        }

        try {
            cx.close();
            getLogger().fine("CLOSE CONNECTION");
        } catch (SQLException e) {
            String msg = "Error occurred closing connection";
            getLogger().warning(msg);

            if (getLogger().isLoggable(Level.FINER)) {
                getLogger().log(Level.FINER, msg, e);
            }
        }
    }

    ////////////////////////////////////////////////////////////////////////////
    // SQL utils ///////////////////////////////////////////////////////////////
    ////////////////////////////////////////////////////////////////////////////


    /**
     * {@inheritDoc }
     */
    @Override
    public FilterToSQL createFilterToSQL(final FeatureType featureType) {
        return initializeFilterToSQL(dialect.createFilterToSQL(), (SimpleFeatureType)featureType);
    }

    /**
     * Creates a new instance of a filter to sql encoder to be
     * used in a prepared statement.
     */
    protected PreparedFilterToSQL createPreparedFilterToSQL(final SimpleFeatureType featureType) {
        return initializeFilterToSQL(((PreparedStatementSQLDialect) dialect).createPreparedFilterToSQL(), featureType);
    }

    /**
     * Helper method to initialize a filter encoder instance.
     */
    protected <F extends FilterToSQL> F initializeFilterToSQL(final F toSQL, final SimpleFeatureType featureType) {
        toSQL.setSqlNameEscape(dialect.getNameEscape());

        if (featureType != null) {
            //set up a fid mapper
            //TODO: remove this
            final PrimaryKey key;

            try {
                key = getPrimaryKey(featureType);
            } catch (DataStoreException e) {
                throw new RuntimeException(e);
            }

            FIDMapper mapper = new FIDMapper() {

                @Override
                public String createID(Connection conn, SimpleFeature feature, Statement statement)
                        throws IOException {
                    return null;
                }

                @Override
                public int getColumnCount() {
                    return key.getColumns().size();
                }

                @Override
                public int getColumnDecimalDigits(int colIndex) {
                    return 0;
                }

                @Override
                public String getColumnName(int colIndex) {
                    return key.getColumns().get(colIndex).getName();
                }

                @Override
                public int getColumnSize(int colIndex) {
                    return 0;
                }

                @Override
                public int getColumnType(int colIndex) {
                    return 0;
                }

                @Override
                public String getID(Object[] attributes) {
                    return null;
                }

                @Override
                public Object[] getPKAttributes(String FID) throws IOException {
                    return decodeFID(key, FID, false).toArray();
                }

                @Override
                public boolean hasAutoIncrementColumns() {
                    return false;
                }

                @Override
                public void initSupportStructures() {
                }

                @Override
                public boolean isAutoIncrement(int colIndex) {
                    return false;
                }

                @Override
                public boolean isVolatile() {
                    return false;
                }

                @Override
                public boolean returnFIDColumnsAsAttributes() {
                    return false;
                }

                @Override
                public boolean isValid(String fid) {
                    return true;
                }
            };
            toSQL.setFeatureType(featureType);
            toSQL.setFIDMapper(mapper);
        }

        return toSQL;
    }

    /**
     * Generates a 'CREATE TABLE' sql statement.
     */
    protected String createTableSQL(final SimpleFeatureType featureType, final Connection cx) throws SQLException {
        //figure out the names and types of the columns
        final String tableName = featureType.getTypeName();
        final int size = featureType.getAttributeCount();
        final String[] columnNames = new String[size];
        final Class[] classes = new Class[size];
        final boolean[] nillable = new boolean[size];
        final List<String> pkeyColumn = new ArrayList<String>();

        for (int i=0; i<size; i++) {
            final AttributeDescriptor desc = featureType.getDescriptor(i);
            columnNames[i] = desc.getLocalName();
            classes[i] = desc.getType().getBinding();
            nillable[i] = desc.getMinOccurs() <= 0 || desc.isNillable();

            if(isPrimaryKey(desc.getUserData())){
                pkeyColumn.add(desc.getLocalName());
            }
        }

        final String[] sqlTypeNames = getSQLTypeNames(classes, cx);

        for (int i=0; i<sqlTypeNames.length; i++) {
            if (sqlTypeNames[i] == null) {
                throw new SQLException("Unable to map " + columnNames[i] + "( " + classes[i].getName() + ")");
            }
        }

        //build the create table sql -------------------------------------------
        final StringBuilder sql = new StringBuilder();
        sql.append("CREATE TABLE ");
        encodeTableName(tableName, sql);
        sql.append(" ( ");

        if(pkeyColumn.isEmpty()){
            //we create a primary key, this will modify the geature type but
            //we don't have any other solution
            dialect.encodeColumnName("fid", sql);
            dialect.encodePrimaryKey(Integer.class,"INTEGER", sql);
            sql.append(", ");
        }

        //normal attributes
        for (int i = 0; i < columnNames.length; i++) {
            final AttributeDescriptor att = featureType.getDescriptor(columnNames[i]);

            //the column name
            dialect.encodeColumnName(columnNames[i], sql);
            sql.append(' ');

            if(pkeyColumn.contains(columnNames[i])){
                dialect.encodePrimaryKey(att.getType().getBinding(), sqlTypeNames[i], sql);

            }else if (sqlTypeNames[i].toUpperCase().startsWith("VARCHAR")) {
                //sql type name
                //JD: some sql dialects require strings / varchars to have an
                // associated size with them
                Integer length = findVarcharColumnLength(att);
                if (length == null || length < 0) {
                    length = 255;
                }
                dialect.encodeColumnType(sqlTypeNames[i] + '(' + length + ')', sql);
            } else {
                dialect.encodeColumnType(sqlTypeNames[i], sql);
            }

            //nullable
            if (nillable != null && !nillable[i]) {
                sql.append(" NOT NULL ");
            }

            //delegate to dialect to encode column postamble
            dialect.encodePostColumnCreateTable(att, sql);

            //sql.append(sqlTypeNames[i]);
            if (i < (sqlTypeNames.length - 1)) {
                sql.append(", ");
            }
        }

        sql.append(" ) ");

        //encode anything post create table
        dialect.encodePostCreateTable(tableName, sql);

        return sql.toString();
    }

    /**
     * Helper method to encode table name which checks if a schema is set and
     * prefixes the table name with it.
     */
    protected void encodeTableName(final String tableName, final StringBuilder sql) {
        if (databaseSchema != null) {
            dialect.encodeSchemaName(databaseSchema, sql);
            sql.append('.');
        }

        dialect.encodeTableName(tableName, sql);
    }


    /**
     * Generates a 'SELECT p1, p2, ... FROM ... WHERE ...' statement.
     *
     * @param featureType
     *            the feature type that the query must return (may contain less
     *            attributes than the native one)
     * @param query
     *            the query to be run. The type name and property will be ignored, as they are
     *            supposed to have been already embedded into the provided feature type
     * @return String
     */
    protected String selectSQL(final SimpleFeatureType featureType, final Query query) throws SQLException,DataStoreException {
        final StringBuilder sql = new StringBuilder("SELECT ");

        final PrimaryKey key = getPrimaryKey(featureType);

        //column names
        for (AttributeDescriptor att : featureType.getAttributeDescriptors()) {
            if (att instanceof GeometryDescriptor) {
                //encode as geometry
                encodeGeometryColumn((GeometryDescriptor) att, sql, query.getHints());

                //alias it to be the name of the original geometry
                dialect.encodeColumnAlias(att.getLocalName(), sql);
            } else {
                dialect.encodeColumnName(att.getLocalName(), sql);
            }

            sql.append(',');
        }

        sql.setLength(sql.length() - 1);

        sql.append(" FROM ");
        encodeTableName(featureType.getTypeName(), sql);

        //filtering
        final Filter filter = query.getFilter();
        if (filter != null && !Filter.INCLUDE.equals(filter)) {
            //encode filter
            try {
                // grab the full feature type, as we might be encoding a filter
                // that uses attributes that aren't returned in the results
                final SimpleFeatureType fullSchema = (SimpleFeatureType) getFeatureType(featureType.getTypeName());
                final FilterToSQL toSQL = createFilterToSQL(fullSchema);
                sql.append(' ').append(toSQL.encodeToString(filter));
            } catch (FilterToSQLException e) {
                throw new SQLException(e);
            }
        }

        //sorting
        sort(featureType, query.getSortBy(), key, sql);

        // finally encode limit/offset, if necessary
        applyLimitOffset(sql, query);

        return sql.toString();
    }


    /**
     * Generates a 'SELECT count(*) FROM' sql statement. In case limit/offset is
     * used, we'll need to apply them on a <code>select *<code>
     * as limit/offset usually alters the number of returned rows
     * (and a count returns just one), and then count on the result of that first select
     */
    protected String selectCountSQL(final SimpleFeatureType featureType, final Query query) throws SQLException {
        final StringBuilder sql = new StringBuilder();

        final boolean limitOffset = checkLimitOffset(query);
        if (limitOffset) {
            sql.append("SELECT * FROM ");
        } else {
            sql.append("SELECT count(*) FROM ");
        }
        encodeTableName(featureType.getTypeName(), sql);

        final Filter filter = query.getFilter();
        if (filter != null && !Filter.INCLUDE.equals(filter)) {
            //encode filter
            try {
                final FilterToSQL toSQL = createFilterToSQL(featureType);
                sql.append(' ').append(toSQL.encodeToString(filter));
            } catch (FilterToSQLException e) {
                throw new SQLException(e);
            }
        }

        if (limitOffset) {
            applyLimitOffset(sql, query);
            sql.insert(0, "SELECT COUNT(*) FROM (");
            sql.append(") AS GT_COUNT_ ");

        }

        return sql.toString();
    }

    /**
     * Generates a 'SELECT count(*) FROM' prepared statement.
     */
    protected PreparedStatement selectCountSQLPS(final SimpleFeatureType featureType, final Query query,
                                                 final Connection cx) throws SQLException{
        final StringBuilder sql = new StringBuilder();

        final boolean limitOffset = checkLimitOffset(query);
        if (limitOffset) {
            sql.append("SELECT * FROM ");
        } else {
            sql.append("SELECT count(*) FROM ");
        }
        encodeTableName(featureType.getTypeName(), sql);

        PreparedFilterToSQL toSQL = null;
        final Filter filter = query.getFilter();
        if (filter != null && !Filter.INCLUDE.equals(filter)) {
            //encode filter
            try {
                toSQL = createPreparedFilterToSQL(featureType);
                sql.append(" ").append(toSQL.encodeToString(filter));
            } catch (FilterToSQLException e) {
                throw new SQLException(e);
            }
        }

        if (limitOffset) {
            applyLimitOffset(sql, query);
            sql.insert(0, "SELECT COUNT(*) FROM (");
            sql.append(")");
            dialect.encodeTableAlias("GT_COUNT_", sql);
        }

        getLogger().fine(sql.toString());
        final PreparedStatement ps = cx.prepareStatement(sql.toString());

        if (toSQL != null) {
            setPreparedFilterValues(ps, toSQL, 0, cx);
        }

        return ps;
    }


    /**
     * Encodes the sort-by portion of an sql query
     * @param featureType
     * @param sort
     * @param key
     * @param sql
     * @throws IOException
     */
    void sort(final SimpleFeatureType featureType, final SortBy[] sort, final PrimaryKey key,
            final StringBuilder sql) throws DataStoreException {
        if ((sort != null) && (sort.length > 0)) {
            sql.append(" ORDER BY ");

            for (final SortBy sortBy : sort) {
                final String order;
                if (sortBy.getSortOrder() == SortOrder.DESCENDING) {
                    order = " DESC";
                } else {
                    order = " ASC";
                }

                if (SortBy.NATURAL_ORDER.equals(sortBy) || SortBy.REVERSE_ORDER.equals(sortBy)) {
                    if (key instanceof NullPrimaryKey) {
                        throw new DataStoreException("Cannot do natural order without a primary key");
                    }

                    for (PrimaryKeyColumn col : key.getColumns()) {
                        dialect.encodeColumnName(col.getName(), sql);
                        sql.append(order);
                        sql.append(',');
                    }
                } else {
                    dialect.encodeColumnName(getPropertyName(featureType, sortBy.getPropertyName()), sql);
                    sql.append(order);
                    sql.append(',');
                }
            }

            sql.setLength(sql.length() - 1);
        }
    }

    /**
     * Generates a 'SELECT p1, p2, ... FROM ... WHERE ...' prepared statement.
     *
     * @param featureType
     *            the feature type that the query must return (may contain less
     *            attributes than the native one)
     * @param query
     *            the query to be run. The type name and property will be ignored, as they are
     *            supposed to have been already embedded into the provided feature type
     * @param cx
     *            The database connection to be used to create the prepared
     *            statement
     */
    protected PreparedStatement selectSQLPS(final SimpleFeatureType featureType, final Query query,
                                            final Connection cx) throws SQLException, DataStoreException{

        final StringBuilder sql = new StringBuilder("SELECT ");

        // primary key
        final PrimaryKey key = getPrimaryKey(featureType);

        //other columns
        for (AttributeDescriptor att : featureType.getAttributeDescriptors()) {
            if (att instanceof GeometryDescriptor) {
                //encode as geometry
                encodeGeometryColumn((GeometryDescriptor) att, sql, query.getHints());

                //alias it to be the name of the original geometry
                dialect.encodeColumnAlias(att.getLocalName(), sql);
            } else {
                dialect.encodeColumnName(att.getLocalName(), sql);
            }

            sql.append(',');
        }

        sql.setLength(sql.length() - 1);

        sql.append(" FROM ");
        encodeTableName(featureType.getTypeName(), sql);

        //filtering
        PreparedFilterToSQL toSQL = null;
        final Filter filter = query.getFilter();
        if (filter != null && !Filter.INCLUDE.equals(filter)) {
            //encode filter
            try {
                // grab the full feature type, as we might be encoding a filter
                // that uses attributes that aren't returned in the results
                final SimpleFeatureType fullSchema = (SimpleFeatureType) getFeatureType(featureType.getTypeName());
                toSQL = createPreparedFilterToSQL(fullSchema);
                sql.append(' ').append(toSQL.encodeToString(filter));
            } catch (FilterToSQLException e) {
                throw new SQLException(e);
            }
        }

        //sorting
        sort(featureType, query.getSortBy(), key, sql);

        // finally encode limit/offset, if necessary
        applyLimitOffset(sql, query);

        getLogger().fine(sql.toString());
        final PreparedStatement ps = cx.prepareStatement(sql.toString(), ResultSet.TYPE_FORWARD_ONLY,
                                                                         ResultSet.CONCUR_READ_ONLY);
        ps.setFetchSize(fetchSize);

        if (toSQL != null) {
            setPreparedFilterValues(ps, toSQL, 0, cx);
        }

        return ps;
    }


    /**
     * Generates a 'SELECT' sql statement which selects bounds.
     *
     * @param featureType The feature type / table.
     * @param query Specifies which features are to be used for the bounds computation
     *              (and in particular uses filter, start index and max features)
     */
    protected String selectBoundsSQL(final SimpleFeatureType featureType, final Query query) throws SQLException {
        final StringBuilder sql = new StringBuilder();

        final boolean offsetLimit = checkLimitOffset(query);
        if (offsetLimit) {
            // envelopes are aggregates, just like count, so we must first isolate
            // the rows against which the aggregate will work in a subquery
            sql.append(" SELECT *");
        } else {
            sql.append("SELECT ");
            buildEnvelopeAggregates(featureType, sql);
        }

        sql.append(" FROM ");
        encodeTableName(featureType.getTypeName(), sql);

        final Filter filter = query.getFilter();
        if (filter != null && !Filter.INCLUDE.equals(filter)) {
            //encode filter
            try {
                final FilterToSQL toSQL = createFilterToSQL(featureType);
                sql.append(" ").append(toSQL.encodeToString(filter));
            } catch (FilterToSQLException e) {
                throw new SQLException(e);
            }
        }

        // finally encode limit/offset, if necessary
        if (offsetLimit) {
            applyLimitOffset(sql, query);
            // build the prologue
            StringBuilder sb = new StringBuilder();
            sb.append("SELECT ");
            buildEnvelopeAggregates(featureType, sb);
            sb.append("FROM (");
            // wrap the existing query
            sql.insert(0, sb.toString());
            sql.append(")");
            dialect.encodeTableAlias("GT2_BOUNDS_", sql);
        }

        return sql.toString();
    }

    /**
     * Generates a 'SELECT' prepared statement which selects bounds.
     *
     * @param featureType The feature type / table.
     * @param query Specifies which features are to be used for the bounds computation
     *              (and in particular uses filter, start index and max features)
     * @param cx A database connection.
     */
    protected PreparedStatement selectBoundsSQLPS(final SimpleFeatureType featureType, final Query query,
                                                  final Connection cx) throws SQLException{

        final StringBuilder sql = new StringBuilder();

        final boolean offsetLimit = checkLimitOffset(query);
        if (offsetLimit) {
            // envelopes are aggregates, just like count, so we must first isolate
            // the rows against which the aggregate will work in a subquery
            sql.append(" SELECT *");
        } else {
            sql.append("SELECT ");
            buildEnvelopeAggregates(featureType, sql);
        }

        sql.append(" FROM ");
        encodeTableName(featureType.getTypeName(), sql);

        // encode the filter
        PreparedFilterToSQL toSQL = null;
        final Filter filter = query.getFilter();
        if (filter != null && !Filter.INCLUDE.equals(filter)) {
            //encode filter
            try {
                toSQL = createPreparedFilterToSQL(featureType);
                sql.append(" ").append(toSQL.encodeToString(filter));
            } catch (FilterToSQLException e) {
                throw new SQLException(e);
            }
        }

        // finally encode limit/offset, if necessary
        if (offsetLimit) {
            applyLimitOffset(sql, query);
            // build the prologue
            final StringBuilder sb = new StringBuilder();
            sb.append("SELECT ");
            buildEnvelopeAggregates(featureType, sb);
            sb.append("FROM (");
            // wrap the existing query
            sql.insert(0, sb.toString());
            sql.append(")");
            dialect.encodeTableAlias("GT2_BOUNDS_", sql);
        }


        getLogger().fine(sql.toString());
        final PreparedStatement ps = cx.prepareStatement(sql.toString());

        if (toSQL != null) {
            setPreparedFilterValues(ps, toSQL, 0, cx);
        }

        return ps;
    }

    /**
     * Builds a list of the aggregate function calls necesary to compute each geometry
     * column bounds
     * @param featureType
     * @param sql
     */
    private void buildEnvelopeAggregates(final SimpleFeatureType featureType, final StringBuilder sql) {
        //walk through all geometry attributes and build the query
        for (final Iterator a = featureType.getAttributeDescriptors().iterator(); a.hasNext();) {
            final AttributeDescriptor attribute = (AttributeDescriptor) a.next();
            if (attribute instanceof GeometryDescriptor) {
                final String geometryColumn = featureType.getGeometryDescriptor().getLocalName();
                dialect.encodeGeometryEnvelope(featureType.getTypeName(), geometryColumn, sql);
                sql.append(",");
            }
        }
        sql.setLength(sql.length() - 1);
    }

    /**
     * Applies the limit/offset elements to the query if they are specified
     * and if the dialect supports them
     * @param sql The sql to be modified
     * @param the query that holds the limit and offset parameters
     */
    private void applyLimitOffset(final StringBuilder sql, final Query query) {
        if (checkLimitOffset(query)) {
            final int offset = query.getStartIndex();
            final Integer limit = query.getMaxFeatures();
            dialect.applyLimitOffset(sql, limit, offset);
        }
    }

    /**
     * Checks if the query needs limit/offset treatment
     * @param query
     * @return true if the query needs limit/offset treatment and if the sql dialect can do that natively
     */
    private boolean checkLimitOffset(final Query query) {
        // if we cannot, don't bother checking the query
        if (!dialect.isLimitOffsetSupported()) {
            return false;
        }

        // the check the query has at least a non default value for limit/offset
        final int offset = query.getStartIndex();
        final Integer limit = query.getMaxFeatures();
        return limit != null || offset > 0;
    }

    /**
     * Helper method for setting the values of the WHERE class of a prepared statement.
     *
     */
    protected void setPreparedFilterValues(final PreparedStatement ps, final PreparedFilterToSQL toSQL,
                                           final int offset, final Connection cx) throws SQLException{
        final PreparedStatementSQLDialect dialect = (PreparedStatementSQLDialect) getDialect();

        for (int i = 0; i < toSQL.getLiteralValues().size(); i++) {
            final Object value = toSQL.getLiteralValues().get(i);
            final Class binding = toSQL.getLiteralTypes().get(i);
            Integer srid = toSQL.getSRIDs().get(i);
            if (srid == null) {
                srid = -1;
            }

            if (binding != null && Geometry.class.isAssignableFrom(binding)) {
                dialect.setGeometryValue((Geometry) value, srid, binding, ps, offset + i + 1);
            } else {
                dialect.setValue(value, binding, ps, offset + i + 1, cx);
            }
            if (getLogger().isLoggable(Level.FINE)) {
                getLogger().fine((i + 1) + " = " + value);
            }
        }
    }


    /**
     * Helper method for determining what the sql type names are for a set of
     * classes.
     * <p>
     * This method uses a combination of dialect mappings and database metadata
     * to determine which sql types map to the specified classes.
     * </p>
     */
    private String[] getSQLTypeNames(final Class[] classes, final Connection cx)
            throws SQLException {
        //figure out what the sql types are corresponding to the feature type
        // attributes
        final int[] sqlTypes = new int[classes.length];
        final String[] sqlTypeNames = new String[sqlTypes.length];

        for (int i = 0; i < classes.length; i++) {
            final Class clazz = classes[i];
            Integer sqlType = dialect.getMapping(clazz);

            if (sqlType == null) {
                getLogger().warning("No sql type mapping for: " + clazz);
                sqlType = Types.OTHER;
            }

            sqlTypes[i] = sqlType;

            //if this a geometric type, get the name from teh dialect
            //if ( attributeType instanceof GeometryDescriptor ) {
            if (Geometry.class.isAssignableFrom(clazz)) {
                String sqlTypeName = dialect.getGeometryTypeName(sqlType);

                if (sqlTypeName != null) {
                    sqlTypeNames[i] = sqlTypeName;
                }
            }

            //check the overrides
            final String sqlTypeName = dialect.getSqlTypeToSqlTypeNameOverrides().get(sqlType);
            if (sqlTypeName != null) {
                sqlTypeNames[i] = sqlTypeName;
            }

        }

        //figure out the type names that correspond to the sql types from
        // the database metadata
        final DatabaseMetaData metaData = cx.getMetaData();

        /*
         *      <LI><B>TYPE_NAME</B> String => Type name
         *        <LI><B>DATA_TYPE</B> int => SQL data type from java.sql.Types
         *        <LI><B>PRECISION</B> int => maximum precision
         *        <LI><B>LITERAL_PREFIX</B> String => prefix used to quote a literal
         *      (may be <code>null</code>)
         *        <LI><B>LITERAL_SUFFIX</B> String => suffix used to quote a literal
        (may be <code>null</code>)
         *        <LI><B>CREATE_PARAMS</B> String => parameters used in creating
         *      the type (may be <code>null</code>)
         *        <LI><B>NULLABLE</B> short => can you use NULL for this type.
         *      <UL>
         *      <LI> typeNoNulls - does not allow NULL values
         *      <LI> typeNullable - allows NULL values
         *      <LI> typeNullableUnknown - nullability unknown
         *      </UL>
         *        <LI><B>CASE_SENSITIVE</B> boolean=> is it case sensitive.
         *        <LI><B>SEARCHABLE</B> short => can you use "WHERE" based on this type:
         *      <UL>
         *      <LI> typePredNone - No support
         *      <LI> typePredChar - Only supported with WHERE .. LIKE
         *      <LI> typePredBasic - Supported except for WHERE .. LIKE
         *      <LI> typeSearchable - Supported for all WHERE ..
         *      </UL>
         *        <LI><B>UNSIGNED_ATTRIBUTE</B> boolean => is it unsigned.
         *        <LI><B>FIXED_PREC_SCALE</B> boolean => can it be a money value.
         *        <LI><B>AUTO_INCREMENT</B> boolean => can it be used for an
         *      auto-increment value.
         *        <LI><B>LOCAL_TYPE_NAME</B> String => localized version of type name
         *      (may be <code>null</code>)
         *        <LI><B>MINIMUM_SCALE</B> short => minimum scale supported
         *        <LI><B>MAXIMUM_SCALE</B> short => maximum scale supported
         *        <LI><B>SQL_DATA_TYPE</B> int => unused
         *        <LI><B>SQL_DATETIME_SUB</B> int => unused
         *        <LI><B>NUM_PREC_RADIX</B> int => usually 2 or 10
         */
        final ResultSet types = metaData.getTypeInfo();

        try {
            while (types.next()) {
                final int sqlType = types.getInt("DATA_TYPE");
                final String sqlTypeName = types.getString("TYPE_NAME");

                for (int i = 0; i < sqlTypes.length; i++) {
                    //check if we already have the type name from the dialect
                    if (sqlTypeNames[i] != null) {
                        continue;
                    }

                    if (sqlType == sqlTypes[i]) {
                        sqlTypeNames[i] = sqlTypeName;
                    }
                }
            }
        } finally {
            closeSafe(types);
        }

        // apply the overrides specified by the dialect
        final Map<Integer, String> overrides = dialect.getSqlTypeToSqlTypeNameOverrides();
        for (int i = 0; i < sqlTypes.length; i++) {
            final String override = overrides.get(sqlTypes[i]);
            if (override != null) {
                sqlTypeNames[i] = override;
            }
        }

        return sqlTypeNames;
    }


    /**
     * Generates a 'DELETE FROM' sql statement.
     */
    protected String deleteSQL(final SimpleFeatureType featureType, final Filter filter) throws SQLException {
        final StringBuilder sql = new StringBuilder("DELETE FROM ");
        encodeTableName(featureType.getTypeName(), sql);

        if (filter != null && !Filter.INCLUDE.equals(filter)) {
            //encode filter
            try {
                final FilterToSQL toSQL = createFilterToSQL(featureType);
                sql.append(" ").append(toSQL.encodeToString(filter));
            } catch (FilterToSQLException e) {
                throw new SQLException(e);
            }
        }

        return sql.toString();
    }

    /**
     * Generates a 'DELETE FROM' prepared statement.
     */
    protected PreparedStatement deleteSQLPS(final SimpleFeatureType featureType, final Filter filter,
                                            final Connection cx) throws SQLException {
        final StringBuilder sql = new StringBuilder("DELETE FROM ");
        encodeTableName(featureType.getTypeName(), sql);

        PreparedFilterToSQL toSQL = null;
        if (filter != null && !Filter.INCLUDE.equals(filter)) {
            //encode filter
            try {
                toSQL = createPreparedFilterToSQL(featureType);
                sql.append(" ").append(toSQL.encodeToString(filter));
            } catch (FilterToSQLException e) {
                throw new SQLException(e);
            }
        }

        getLogger().fine(sql.toString());
        final PreparedStatement ps = cx.prepareStatement(sql.toString());

        if (toSQL != null) {
            setPreparedFilterValues(ps, toSQL, 0, cx);
        }

        return ps;
    }

    /**
     * Generates a 'INSERT INFO' sql statement.
     * @throws IOException
     */
    protected String insertSQL(final SimpleFeatureType featureType, final SimpleFeature feature,
                               final List keyValues, final Connection cx) throws DataStoreException{
        final SQLDialect dialect = getDialect();
        final PrimaryKey key = getPrimaryKey(featureType);
        final List<PrimaryKeyColumn> keyColumns = key.getColumns();

        final StringBuilder sqlType = new StringBuilder();
        sqlType.append("INSERT INTO ");
        encodeTableName(featureType.getTypeName(), sqlType);
        sqlType.append(" ( ");

        final StringBuilder sqlValues = new StringBuilder();
        sqlValues.append(" ) VALUES ( ");

        //add all fields
        fields :
        for(int i=0,n=featureType.getAttributeCount(); i<n; i++){
            final AttributeDescriptor desc = featureType.getDescriptor(i);
            final String attName = desc.getLocalName();
            final Class binding = desc.getType().getBinding();
            final Object value = feature.getAttribute(attName);

            //remove the primary key attribut that wil be auto-generated and null
            for (PrimaryKeyColumn col : keyColumns) {
                if(col.getName().equals(attName)){
                    //only include if its non auto generating and not null
                    if (col instanceof AutoGeneratedPrimaryKeyColumn && value == null) {
                        continue fields;
                    }
                }
            }

            //the column
            dialect.encodeColumnName(attName, sqlType);

            //the value
            if (value == null) {
                //maybe it's an auto generated value from a sequence
                boolean found = false;
                for (int k=0; k<keyColumns.size(); k++) {
                    if(keyColumns.get(k).getName().equals(attName)){
                        dialect.encodeValue(keyValues.get(k), keyColumns.get(k).getType(), sqlValues);
                        found = true;
                        break;
                    }
                }

                if(!found){
                    if (!desc.isNillable()) {
                        //TODO: throw an exception
                    }
                    sqlValues.append("null");
                }
            } else {
                if (Geometry.class.isAssignableFrom(binding)) {
                    final Geometry g = (Geometry) value;
                    final int srid = getGeometrySRID(g, desc);
                    try {
                        dialect.encodeGeometryValue(g, srid, sqlValues);
                    } catch (IOException ex) {
                        throw new DataStoreException(ex);
                    }
                } else {
                    dialect.encodeValue(value, binding, sqlValues);
                }
            }

            sqlType.append(',');
            sqlValues.append(',');
        }

        sqlType.setLength(sqlType.length() - 1);
        sqlValues.setLength(sqlValues.length() - 1);
        sqlValues.append(")");

        return sqlType.toString() + sqlValues.toString();
    }

    /**
     * Generates a 'INSERT INFO' prepared statement.
     */
    protected PreparedStatement insertSQLPS(final SimpleFeatureType featureType, final SimpleFeature feature,
            final List keyValues, final Connection cx) throws IOException, SQLException, DataStoreException{
        final PreparedStatementSQLDialect dialect = (PreparedStatementSQLDialect) getDialect();
        final PrimaryKey key = getPrimaryKey(featureType);
        final List<PrimaryKeyColumn> keyColumns = key.getColumns();

        final StringBuilder sqlType = new StringBuilder();
        sqlType.append("INSERT INTO ");
        encodeTableName(featureType.getTypeName(), sqlType);
        sqlType.append(" ( ");

        final StringBuilder sqlValues = new StringBuilder();
        sqlValues.append(" ) VALUES ( ");

        //add all fields
        fields :
        for(int i=0,n=featureType.getAttributeCount(); i<n; i++){
            final AttributeDescriptor desc = featureType.getDescriptor(i);
            final String attName = desc.getLocalName();
            final Class binding = desc.getType().getBinding();
            final Object value = feature.getAttribute(attName);

            //remove the primary key attribut that wil be auto-generated and null
            for (PrimaryKeyColumn col : keyColumns) {
                if(col.getName().equals(attName)){
                    //only include if its non auto generating and not null
                    if (col instanceof AutoGeneratedPrimaryKeyColumn && value == null) {
                        continue fields;
                    }
                }
            }

            //the column
            dialect.encodeColumnName(attName, sqlType);

            //the value
            if (value == null) {
                sqlValues.append("?");
            } else {
                if (Geometry.class.isAssignableFrom(binding)) {
                    final Geometry g = (Geometry) value;
                    final int srid = getGeometrySRID(g, desc);
                    dialect.prepareGeometryValue(g, srid, binding, sqlValues);
                } else {
                    sqlValues.append("?");
                }
            }

            sqlType.append(',');
            sqlValues.append(',');
        }
        
        sqlType.setLength(sqlType.length() - 1);
        sqlValues.setLength(sqlValues.length() - 1);
        sqlValues.append(")");
        final String request = sqlType.toString() + sqlValues.toString();


        //create the prepared statement-----------------------------------------
        final PreparedStatement ps = cx.prepareStatement(request);

        //fill the prepared statement
        int attIdx = 0;
        fields :
        for(int i=0,n=featureType.getAttributeCount(); i<n; i++){
            final AttributeDescriptor desc = featureType.getDescriptor(i);
            final String attName = desc.getLocalName();
            final Class binding = desc.getType().getBinding();
            final Object value = feature.getAttribute(attName);

            //remove the primary key attribut that wil be auto-generated and null
            for (PrimaryKeyColumn col : keyColumns) {
                if(col.getName().equals(attName)){
                    //only include if its non auto generating and not null
                    if (col instanceof AutoGeneratedPrimaryKeyColumn && value == null) {
                        continue fields;
                    }
                }
            }

            attIdx++;

            //the column
            dialect.encodeColumnName(attName, sqlType);

            //the value
            if (value == null) {
                //maybe it's an auto generated value from a sequence
                boolean found = false;
                for (int k=0; k<keyColumns.size(); k++) {
                    if(keyColumns.get(k).getName().equals(attName)){
                        dialect.setValue(keyValues.get(k), keyColumns.get(k).getType(), ps, attIdx, cx);
                        found = true;
                        break;
                    }
                }

                if(!found){
                    if (!desc.isNillable()) {
                        //TODO: throw an exception
                    }
                    dialect.setValue(value, binding, ps, attIdx, cx);
                }
            } else {
                if (Geometry.class.isAssignableFrom(binding)) {
                    final Geometry g = (Geometry) value;
                    final int srid = getGeometrySRID(g, desc);
                    dialect.setGeometryValue(g, srid, binding, ps, attIdx);
                } else {
                    dialect.setValue(value, binding, ps, attIdx, cx);
                }
            }

            sqlType.append(',');
            sqlValues.append(',');
        }

        return ps;
    }

    /**
     * Generates an 'UPDATE' sql statement.
     */
    protected String updateSQL(final SimpleFeatureType featureType, final AttributeDescriptor[] attributes,
            final Object[] values, final Filter filter) throws IOException, SQLException{
        final SQLDialect dialect = getDialect();

        final StringBuilder sql = new StringBuilder();
        sql.append("UPDATE ");
        encodeTableName(featureType.getTypeName(), sql);

        sql.append(" SET ");

        for (int i = 0; i < attributes.length; i++) {
            dialect.encodeColumnName(attributes[i].getLocalName(), sql);
            sql.append(" = ");

            if (Geometry.class.isAssignableFrom(attributes[i].getType().getBinding())) {
                    final Geometry g = (Geometry) values[i];
                    final int srid = getGeometrySRID(g, attributes[i]);
                    dialect.encodeGeometryValue(g, srid, sql);
            } else {
                dialect.encodeValue(values[i], attributes[i].getType().getBinding(), sql);
            }

            sql.append(",");
        }

        sql.setLength(sql.length() - 1);
        sql.append(" ");

        if (filter != null && !Filter.INCLUDE.equals(filter)) {
            //encode filter
            try {
                final FilterToSQL toSQL = createFilterToSQL(featureType);
                sql.append(" ").append(toSQL.encodeToString(filter));
            } catch (FilterToSQLException e) {
                throw new SQLException(e);
            }
        }

        return sql.toString();
    }

    /**
     * Generates an 'UPDATE' prepared statement.
     */
    protected PreparedStatement updateSQLPS(final SimpleFeatureType featureType,
            final AttributeDescriptor[] attributes, final Object[] values, final Filter filter,
            final Connection cx) throws IOException, SQLException{
        final PreparedStatementSQLDialect dialect = (PreparedStatementSQLDialect) getDialect();

        final StringBuilder sql = new StringBuilder();
        sql.append("UPDATE ");
        encodeTableName(featureType.getTypeName(), sql);

        sql.append(" SET ");

        for (int i = 0; i < attributes.length; i++) {
            final AttributeDescriptor att = attributes[i];
            dialect.encodeColumnName(att.getLocalName(), sql);
            sql.append(" = ");

            // geometries might need special treatment, delegate to the dialect
            if (attributes[i] instanceof GeometryDescriptor) {
                final Geometry geometry = (Geometry) values[i];
                final Class<?> binding = att.getType().getBinding();
                dialect.prepareGeometryValue(geometry, getDescriptorSRID(att), binding, sql);
            } else {
                sql.append("?");
            }
            sql.append(",");
        }
        sql.setLength(sql.length() - 1);
        sql.append(" ");

        PreparedFilterToSQL toSQL = null;
        if (filter != null && !Filter.INCLUDE.equals(filter)) {
            //encode filter
            try {
                toSQL = createPreparedFilterToSQL(featureType);
                sql.append(" ").append(toSQL.encodeToString(filter));
            } catch (FilterToSQLException e) {
                throw new SQLException(e);
            }
        }

        final PreparedStatement ps = cx.prepareStatement(sql.toString());
        getLogger().log(Level.FINE, "Updating features with prepared statement: {0}", sql);

        int i = 0;
        for (; i < attributes.length; i++) {
            final AttributeDescriptor att = attributes[i];
            final Class binding = att.getType().getBinding();
            if (Geometry.class.isAssignableFrom(binding)) {
                final Geometry g = (Geometry) values[i];
                dialect.setGeometryValue(g, getDescriptorSRID(att), binding, ps, i + 1);
            } else {
                dialect.setValue(values[i], binding, ps, i + 1, cx);
            }
            if (getLogger().isLoggable(Level.FINE)) {
                getLogger().fine((i + 1) + " = " + values[i]);
            }
        }

        if (toSQL != null) {
            setPreparedFilterValues(ps, toSQL, i, cx);
        //for ( int j = 0; j < toSQL.getLiteralValues().size(); j++, i++)  {
        //    Object value = toSQL.getLiteralValues().get( j );
        //    Class binding = toSQL.getLiteralTypes().get( j );
        //
        //    dialect.setValue( value, binding, ps, i+1, cx );
        //    if ( getLogger().isLoggable( Level.FINE ) ) {
        //        getLogger().fine( (i+1) + " = " + value );
        //}
        }

        return ps;
    }

    /**
     * Gets the next value of a primary key.
     */
    protected List<Object> getNextValues(final PrimaryKey pkey, final Connection cx)
            throws SQLException, IOException {
        final ArrayList<Object> next = new ArrayList<Object>();
        for (PrimaryKeyColumn col : pkey.getColumns()) {
            next.add(getNextValue(col, pkey, cx));
        }
        return next;
    }

    /**
     * Gets the next value for the column of a primary key.
     */
    protected Object getNextValue(final PrimaryKeyColumn col, final PrimaryKey pkey, final Connection cx)
            throws SQLException, IOException {
        Object next = null;

        if (col instanceof AutoGeneratedPrimaryKeyColumn) {
            next = dialect.getNextAutoGeneratedValue(databaseSchema, pkey.getTableName(), col.getName(), cx);
        } else if (col instanceof SequencedPrimaryKeyColumn) {
            final String sequenceName = ((SequencedPrimaryKeyColumn) col).getSequenceName();
            next = dialect.getNextSequenceValue(databaseSchema, sequenceName, cx);
        } else {
            //try to calculate
            final Class t = col.getType();

            //is the column numeric?
            if (Number.class.isAssignableFrom(t)) {
                //is the column integral?
                if (t == Short.class || t == Integer.class || t == Long.class || BigInteger.class.isAssignableFrom(t) ||
                                                                                 BigDecimal.class.isAssignableFrom(t))
                {

                    final StringBuilder sql = new StringBuilder();
                    sql.append("SELECT MAX(");
                    dialect.encodeColumnName(col.getName(), sql);
                    sql.append(") + 1 FROM ");
                    encodeTableName(pkey.getTableName(), sql);

                    getLogger().log(Level.FINE, "Getting next FID: {0}", sql);

                    final Statement st = cx.createStatement();
                    try {
                        final ResultSet rs = st.executeQuery(sql.toString());
                        try {
                            rs.next();
                            next = rs.getObject(1);

                            if (next == null) {
                                //this probably means there was no data in the table, set to 1
                                //TODO: probably better to do a count to check... but if this
                                // value already exists the db will throw an error when it tries
                                // to insert
                                next = 1;
                            }
                        } finally {
                            closeSafe(rs);
                        }
                    } finally {
                        closeSafe(st);
                    }
                }
            } else if (CharSequence.class.isAssignableFrom(t)) {
                //generate a random string
                next = SimpleFeatureBuilder.createDefaultFeatureId();
            }

            if (next == null) {
                throw new IOException("Cannot generate key value for column of type: " + t.getName());
            }
        }

        return next;
    }


    ////////////////////////////////////////////////////////////////////////////
    // other utils /////////////////////////////////////////////////////////////
    ////////////////////////////////////////////////////////////////////////////


    /**
     * The filter capabilities which reports which spatial operations the
     * underlying database can handle natively.
     *
     * @return The filter capabilities, never <code>null</code>.
     */
    public DefaultFilterCapabilities getFilterCapabilities() {
        if (dialect instanceof PreparedStatementSQLDialect) {
            return ((PreparedStatementSQLDialect) dialect).createPreparedFilterToSQL().getCapabilities();
        } else {
            return dialect.createFilterToSQL().getCapabilities();
        }
    }

    /**
     * Helper method for splitting a filter.
     */
    Filter[] splitFilter(final Filter original, FeatureType schema) throws DataStoreException {
        final Filter[] split = new Filter[2];
        if ( original != null ) {
            //create a filter splitter
            final CapabilitiesFilterSplitter splitter = new CapabilitiesFilterSplitter(getFilterCapabilities(),
                    schema, null);
            original.accept(splitter, null);

            split[0] = splitter.getFilterPre();
            split[1] = splitter.getFilterPost();
        }

        final SimplifyingFilterVisitor visitor = new SimplifyingFilterVisitor();
        final PrimaryKey key = getPrimaryKey(schema);
        visitor.setFIDValidator( new PrimaryKeyFIDValidator( this,key ) );
        split[0] = (Filter) split[0].accept(visitor, null);
        split[1] = (Filter) split[1].accept(visitor, null);

        return split;
    }

    /**
     * Searches the attribute descriptor restrictions in an attempt to determine
     * the length of the specified varchar column.
     */
    private static Integer findVarcharColumnLength(final AttributeDescriptor att) {
        for (final Filter r : att.getType().getRestrictions()) {
            if (r instanceof PropertyIsLessThanOrEqualTo) {
                final PropertyIsLessThanOrEqualTo c = (PropertyIsLessThanOrEqualTo) r;
                if (c.getExpression1() instanceof Function &&
                        ((Function) c.getExpression1()).getName().toLowerCase().endsWith("length")) {
                    if (c.getExpression2() instanceof Literal) {
                        final Integer length = c.getExpression2().evaluate(null, Integer.class);
                        if (length != null) {
                            return length;
                        }
                    }
                }
            }
        }

        return null;
    }


    @Override
    protected void finalize() throws Throwable {
        if (source != null) {
            getLogger().severe("There's code using JDBC based datastore and " +
                    "not disposing them. This may lead to temporary loss of database connections. " +
                    "Please make sure all data access code calls DataStore.dispose() " +
                    "before freeing all references to it");
            dispose();
        }
        super.finalize();
    }

    @Override
    public void dispose() {
        if (source instanceof ManageableDataSource) {
            try {
                final ManageableDataSource mds = (ManageableDataSource) source;
                source = null;
                mds.close();
            } catch (SQLException e) {
                // it's ok, we did our best..
                getLogger().log(Level.FINE, "Could not close dataSource", e);
            }
        }
    }

    /**
     * Check if the property descriptor is defined as a primary key.
     * @param params
     * @return
     */
    protected boolean isPrimaryKey(final Map<Object,Object> params){
        if(params == null){
            return false;
        }
        
        final Boolean primary = (Boolean) params.get(HintsPending.PROPERTY_IS_IDENTIFIER);
        if(primary != null) return primary;

        return false;
    }

    /**
     * Encoding a geometry column with respect to hints
     * Supported Hints are provided by {@link SQLDialect#addSupportedHints(Set)}
     *
     * @param gatt
     * @param sql
     * @param hints , may be null
     */
    protected void encodeGeometryColumn(final GeometryDescriptor gatt, final StringBuilder sql,
                                        final Hints hints){
        final int srid = getDescriptorSRID(gatt);
        dialect.encodeGeometryColumn(gatt, srid, sql);
    }


    /**
     * Looks up the geometry srs by trying a number of heuristics. Returns -1 if all attempts
     * at guessing the srid failed.
     */
    protected static int getGeometrySRID(final Geometry g, final AttributeDescriptor descriptor) {
        int srid = getDescriptorSRID(descriptor);

        if (g == null) {
            return srid;
        }

        // check for srid in the jts geometry then
        if (srid <= 0 && g.getSRID() > 0) {
            srid = g.getSRID();
        }

        // check if the geometry has anything
        if (srid <= 0) {
            // check for crs object
            final CoordinateReferenceSystem crs = (CoordinateReferenceSystem) g.getUserData();

            if (crs != null) {
                try {
                    final Integer candidate = CRS.lookupEpsgCode(crs, false);
                    if (candidate != null) {
                        srid = candidate;
                    }
                } catch (Exception e) {
                    // ok, we tried...
                }
            }
        }

        return srid;
    }

    /**
     * Extracts the eventual native SRID user property from the descriptor,
     * returns -1 if not found
     * @param descriptor
     */
    protected static int getDescriptorSRID(final AttributeDescriptor descriptor) {
        // check if we have stored the native srid in the descriptor (we should)
        if (descriptor.getUserData().get(JDBCDataStore.JDBC_NATIVE_SRID) != null) {
            return (Integer) descriptor.getUserData().get(JDBCDataStore.JDBC_NATIVE_SRID);
        }else{
            return -1;
        }
    }

    /**
     * Helper method for executing a property name against a feature type.
     * <p>
     * This method will fall back on {@link PropertyName#getPropertyName()} if
     * it does not evaulate against the feature type.
     * </p>
     */
    protected static String getPropertyName(final SimpleFeatureType featureType, final PropertyName propertyName) {
        final AttributeDescriptor att = (AttributeDescriptor) propertyName.evaluate(featureType);

        if (att != null) {
            return att.getLocalName();
        }

        return propertyName.getPropertyName();
    }

    /**
     * Encodes a feature id from a primary key and result set values.
     */
    public String encodeFID(final PrimaryKey pkey, final ResultSet rs)
            throws SQLException{
        // no pk columns
        if (pkey.getColumns().isEmpty()) {
            return SimpleFeatureBuilder.createDefaultFeatureId();
        }

        // just one, no need to build support structures
        if (pkey.getColumns().size() == 1) {
            return rs.getString(1);
        }

        // more than one
        final List<Object> keyValues = new ArrayList<Object>();
        for (int i = 0; i < pkey.getColumns().size(); i++) {
            String o = rs.getString(i + 1);
            keyValues.add(o);
        }
        return encodeFID(keyValues);
    }

    public String encodeFID(final List<Object> keyValues) {
        final StringBuilder fid = new StringBuilder();
        for (Object o : keyValues) {
            fid.append(o).append('.');
        }
        fid.setLength(fid.length() - 1);
        return fid.toString();
    }

    /**
     * Decodes a fid into its components based on a primary key.
     *
     * @param strict If set to true the value of the fid will be validated against
     *   the type of the key columns. If a conversion can not be made, an exception will be thrown.
     */
    public List<Object> decodeFID(final PrimaryKey key, String FID, final boolean strict) {
        //strip off the feature type name
        if (FID.startsWith(key.getTableName() + '.')) {
            FID = FID.substring(key.getTableName().length() + 1);
        }

        try {
            FID = URLDecoder.decode(FID, "UTF-8");
        } catch (UnsupportedEncodingException e) {
            // Should never occur, because we asked for UTF-8 which is
            // known to be supported.
            throw new AssertionError(e);
        }

        //check for case of multi column primary key and try to backwards map using
        // "." as a seperator of values
        final List values;
        if (key.getColumns().size() > 1) {
            final String[] split = FID.split("\\.");

            //copy over to avoid array store exception
            //values = new ArrayList(split.length);

            //can not do this or it will be a typed list which will raise errors a bit later.
            //values = Arrays.asList(split);
            values = new ArrayList(split.length);
            for(Object o : split) values.add(o);
        } else {
            //single value case
            values = new ArrayList();
            values.add(FID);
        }
        if (values.size() != key.getColumns().size()) {
            throw new IllegalArgumentException("Illegal fid: " + FID + ". Expected " +
                    key.getColumns().size() + " values but got " + values.size());
        }

        //convert to the type of the key
        //JD: usually this would be done by the dialect directly when the value
        // actually gets set but the FIDMapper interface does not report types
        for (int i = 0; i < values.size(); i++) {
            final Object value = values.get(i);
            if (value != null) {
                final Class type = key.getColumns().get(i).getType();
                final Object converted = Converters.convert(value, type);
                if (converted != null) {
                    values.set(i, converted);
                }
                if (strict && !type.isInstance(converted)) {
                    throw new IllegalArgumentException("Value " + values.get(i) + " illegal for type " + type.getName());
                }
            }
        }

        return values;
    }

}
