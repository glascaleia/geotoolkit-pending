/*
 *    Geotoolkit - An Open Source Java GIS Toolkit
 *    http://www.geotoolkit.org
 *
 *    (C) 2002-2008, Open Source Geospatial Foundation (OSGeo)
 *    (C) 2009-2010, Geomatys
 *
 *    This library is free software; you can redistribute it and/or
 *    modify it under the terms of the GNU Lesser General Public
 *    License as published by the Free Software Foundation;
 *    version 2.1 of the License.
 *
 *    This library is distributed in the hope that it will be useful,
 *    but WITHOUT ANY WARRANTY; without even the implied warranty of
 *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 *    Lesser General Public License for more details.
 */
package org.geotoolkit.jdbc;

import com.vividsolutions.jts.geom.Geometry;

import java.io.Closeable;
import java.io.IOException;
import java.math.BigDecimal;
import java.math.BigInteger;
import java.sql.Connection;
import java.sql.DatabaseMetaData;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Set;
import java.util.logging.Level;

import org.geotoolkit.feature.SchemaException;
import org.geotoolkit.jdbc.fid.PrimaryKey;
import org.geotoolkit.jdbc.fid.PrimaryKeyColumn;
import org.geotoolkit.jdbc.fid.PrimaryKeyFIDValidator;
import org.geotoolkit.jdbc.fid.NullPrimaryKey;
import org.geotoolkit.jdbc.dialect.PreparedStatementSQLDialect;
import org.geotoolkit.jdbc.dialect.SQLDialect;
import org.geotoolkit.storage.DataStoreException;
import org.geotoolkit.data.DataStoreRuntimeException;
import org.geotoolkit.data.DataUtilities;
import org.geotoolkit.data.FeatureReader;
import org.geotoolkit.data.FeatureWriter;
import org.geotoolkit.data.jdbc.FilterToSQL;
import org.geotoolkit.data.memory.GenericFilterFeatureIterator;
import org.geotoolkit.data.memory.GenericReprojectFeatureIterator;
import org.geotoolkit.data.memory.GenericRetypeFeatureIterator;
import org.geotoolkit.data.query.DefaultQueryCapabilities;
import org.geotoolkit.data.query.Join;
import org.geotoolkit.data.query.Query;
import org.geotoolkit.data.query.QueryBuilder;
import org.geotoolkit.data.query.QueryCapabilities;
import org.geotoolkit.data.query.Selector;
import org.geotoolkit.data.query.Source;
import org.geotoolkit.factory.Hints;
import org.geotoolkit.factory.HintsPending;
import org.geotoolkit.feature.AttributeDescriptorBuilder;
import org.geotoolkit.feature.AttributeTypeBuilder;
import org.geotoolkit.feature.DefaultName;
import org.geotoolkit.feature.simple.SimpleFeatureBuilder;
import org.geotoolkit.feature.FeatureTypeBuilder;
import org.geotoolkit.filter.capability.DefaultFilterCapabilities;
import org.geotoolkit.filter.visitor.CapabilitiesFilterSplitter;
import org.geotoolkit.filter.visitor.FilterAttributeExtractor;
import org.geotoolkit.filter.visitor.SimplifyingFilterVisitor;
import org.geotoolkit.geometry.jts.JTSEnvelope2D;
import org.geotoolkit.jdbc.dialect.SQLQueryBuilder;
import org.geotoolkit.jdbc.fid.AutoGeneratedPrimaryKeyColumn;
import org.geotoolkit.jdbc.fid.NonIncrementingPrimaryKeyColumn;
import org.geotoolkit.jdbc.fid.SequencedPrimaryKeyColumn;
import org.geotoolkit.referencing.CRS;

import org.opengis.feature.Feature;
import org.opengis.feature.simple.SimpleFeature;
import org.opengis.feature.simple.SimpleFeatureType;
import org.opengis.feature.type.AttributeDescriptor;
import org.opengis.feature.type.FeatureType;
import org.opengis.feature.type.GeometryDescriptor;
import org.opengis.feature.type.Name;
import org.opengis.feature.type.PropertyDescriptor;
import org.opengis.filter.Filter;
import org.opengis.filter.expression.PropertyName;
import org.opengis.filter.identity.FeatureId;
import org.opengis.geometry.Envelope;
import org.opengis.util.FactoryException;
import org.opengis.referencing.crs.CoordinateReferenceSystem;

import static org.geotoolkit.jdbc.MetaDataConstants.*;

/**
 * @author Johann Sorel (Geomatys)
 *
 * @module pending
 */
public final class DefaultJDBCDataStore extends AbstractJDBCDataStore {

    /**
     * writer flags
     */
    protected final int WRITER_ADD = 0x01<<0;
    protected final int WRITER_UPDATE = 0x01<<1;

    private final QueryCapabilities capabilities = new DefaultQueryCapabilities(true);
    private final Map<Name,FeatureType> names = new HashMap<Name, FeatureType>();
    private final Map<Name,PrimaryKey> primaryKeys = new HashMap<Name, PrimaryKey>();
    private final SQLQueryBuilder queryBuilder = new SQLQueryBuilder(this);
    private Set<Name> nameCache = null;

    DefaultJDBCDataStore(String namespace){
        super(namespace);
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public PrimaryKey getPrimaryKey(Name featureTypeName) throws DataStoreException{
        if(nameCache == null){
            visitTables();
        }
        return primaryKeys.get(featureTypeName);
    }

    @Override
    public boolean isWritable(Name typeName) throws DataStoreException {
        final PrimaryKey key = getPrimaryKey(typeName);
        return key != null && !(key instanceof NullPrimaryKey);
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public synchronized Set<Name> getNames() throws DataStoreException {
        Set<Name> ref = nameCache;
        if(ref == null){
            visitTables();
            ref = Collections.unmodifiableSet(new HashSet<Name>(names.keySet()));
            nameCache = ref;
        }
        return ref;
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public FeatureType getFeatureType(Name typeName) throws DataStoreException {
        typeCheck(typeName);
        return names.get(typeName);
    }

    /**
     * Explore the available tables and generate schemas and primary keys.
     * @throws DataStoreException
     */
    private synchronized void visitTables() throws DataStoreException{

        //clear previous schemas, this might be called after an update schema
        nameCache = null;
        names.clear();
        primaryKeys.clear();

        /*
         *        <LI><B>TABLE_CAT</B> String => table catalog (may be <code>null</code>)
         *        <LI><B>TABLE_SCHEM</B> String => table schema (may be <code>null</code>)
         *        <LI><B>TABLE_NAME</B> String => table name
         *        <LI><B>TABLE_TYPE</B> String => table type.  Typical types are "TABLE",
         *                        "VIEW",        "SYSTEM TABLE", "GLOBAL TEMPORARY",
         *                        "LOCAL TEMPORARY", "ALIAS", "SYNONYM".
         *        <LI><B>REMARKS</B> String => explanatory comment on the table
         *  <LI><B>TYPE_CAT</B> String => the types catalog (may be <code>null</code>)
         *  <LI><B>TYPE_SCHEM</B> String => the types schema (may be <code>null</code>)
         *  <LI><B>TYPE_NAME</B> String => type name (may be <code>null</code>)
         *  <LI><B>SELF_REFERENCING_COL_NAME</B> String => name of the designated
         *                  "identifier" column of a typed table (may be <code>null</code>)
         *        <LI><B>REF_GENERATION</B> String => specifies how values in
         *                  SELF_REFERENCING_COL_NAME are created. Values are
         *                  "SYSTEM", "USER", "DERIVED". (may be <code>null</code>)
         */
        Connection cx = null;
        try {
            cx = createConnection();
            final DatabaseMetaData metaData = cx.getMetaData();
            final ResultSet tables = metaData.getTables(null, databaseSchema, "%", new String[]{MD_TABLE, MD_VIEW});

            try {
                while (tables.next()) {
                    final String schemaName = tables.getString(MD_TABLE_SCHEM);
                    final String tableName = tables.getString(MD_TABLE_NAME);

                    //use the dialect to filter
                    if (!dialect.includeTable(schemaName, tableName, cx)) {
                        continue;
                    }

                    final Name name = new DefaultName(getNamespaceURI(), tableName);
                    final SimpleFeatureType sft = createFeatureType(name);
                    final PrimaryKey pkey = createPrimaryKey(name);
                    names.put(name, sft);
                    primaryKeys.put(name, pkey);
                }
            } finally {
                closeSafe(tables);
            }
        } catch (SQLException e) {
            throw new DataStoreException("Error occurred getting table name list.", e);
        } finally {
            closeSafe(cx);
        }

    }

    /**
     * Builds the feature type from database metadata.
     */
    private SimpleFeatureType createFeatureType(Name typeName) throws DataStoreException {
        final FeatureTypeBuilder tb = new FeatureTypeBuilder();
        final AttributeDescriptorBuilder adb = new AttributeDescriptorBuilder();
        final AttributeTypeBuilder atb = new AttributeTypeBuilder();

        //set up the name
        final String tableName = typeName.getLocalPart();
        final String namespace = typeName.getNamespaceURI();
        tb.setName(typeName);

        //ensure we have a connection
        Connection cx = null;
        ResultSet columns = null;
        try {
            /* Get metadata about columns from database.
             *
             *        <LI><B>COLUMN_NAME</B> String => column name
             *        <LI><B>DATA_TYPE</B> int => SQL type from java.sql.Types
             *        <LI><B>TYPE_NAME</B> String => Data source dependent type name,
             *  for a UDT the type name is fully qualified
             *        <LI><B>COLUMN_SIZE</B> int => column size.  For char or date
             *            types this is the maximum number of characters, for numeric or
             *            decimal types this is precision.
             *        <LI><B>BUFFER_LENGTH</B> is not used.
             *        <LI><B>DECIMAL_DIGITS</B> int => the number of fractional digits
             *        <LI><B>NUM_PREC_RADIX</B> int => Radix (typically either 10 or 2)
             *        <LI><B>NULLABLE</B> int => is NULL allowed.
             *      <UL>
             *      <LI> columnNoNulls - might not allow <code>NULL</code> values
             *      <LI> columnNullable - definitely allows <code>NULL</code> values
             *      <LI> columnNullableUnknown - nullability unknown
             *      </UL>
             *         <LI><B>COLUMN_DEF</B> String => default value (may be <code>null</code>)
             *        <LI><B>IS_NULLABLE</B> String => "NO" means column definitely
             *      does not allow NULL values; "YES" means the column might
             *      allow NULL values.  An empty string means nobody knows.
             */

            cx = createConnection();
            final DatabaseMetaData metaData = cx.getMetaData();
            columns = metaData.getColumns(null, databaseSchema, tableName, "%");

            columnIte :
            while (columns.next()) {
                adb.reset();
                atb.reset();

                final String name = columns.getString(MD_COLUMN_NAME);

                //we need the primary keys as fields since join query rely on them.
//                    //encomment to not include primary key in the type
//                    /*
//                     *        <LI><B>TABLE_CAT</B> String => table catalog (may be <code>null</code>)
//                     *        <LI><B>TABLE_SCHEM</B> String => table schema (may be <code>null</code>)
//                     *        <LI><B>TABLE_NAME</B> String => table name
//                     *        <LI><B>COLUMN_NAME</B> String => column name
//                     *        <LI><B>KEY_SEQ</B> short => sequence number within primary key
//                     *        <LI><B>PK_NAME</B> String => primary key name (may be <code>null</code>)
//                     */
//                    final ResultSet primaryKeys = metaData.getPrimaryKeys(null, databaseSchema, tableName);
//                    try {
//                        while (primaryKeys.next()) {
//                            if (name.equals(primaryKeys.getString(MD_COLUMN_NAME))) {
//                                continue columnIte;
//                            }
//                        }
//                    } finally {
//                        closeSafe(primaryKeys);
//                    }

                //figure out the type mapping

                //first ask the dialect
                Class binding = dialect.getMapping(columns, cx);

                if (binding == null) {
                    //determine from type mappings
                    final int dataType = columns.getInt(MD_DATA_TYPE);
                    binding = dialect.getMapping(dataType);
                }

                if (binding == null) {
                    //determine from type name mappings
                    final String tn = columns.getString(MD_TYPE_NAME);
                    binding = dialect.getMapping(tn);
                }

                //if still not found, resort to Object
                if (binding == null) {
                    getLogger().warning("Could not find mapping for:" + name);
                    binding = Object.class;
                }

                adb.setMinOccurs(1);
                //nullability
                if ( MD_NOT_NULL.equalsIgnoreCase( columns.getString(MD_IS_NULLABLE) ) ) {
                    adb.setNillable(false);
                }else{
                    adb.setNillable(true);
                }

                //primary key never null, min one
                final ResultSet primaryKeys = metaData.getPrimaryKeys(null, databaseSchema, tableName);
                try {
                    while (primaryKeys.next()) {
                        if (name.equals(primaryKeys.getString(MD_COLUMN_NAME))) {
                            adb.setNillable(false);
                            adb.setMinOccurs(1);
                            adb.addUserData(HintsPending.PROPERTY_IS_IDENTIFIER, Boolean.TRUE);
                            break;
                        }
                    }
                } finally {
                    closeSafe(primaryKeys);
                }


                //determine if this attribute is a geometry or not
                if (Geometry.class.isAssignableFrom(binding)) {
                    //add the attribute as a geometry, try to figure out
                    // its srid first
                    Integer srid = null;
                    CoordinateReferenceSystem crs = null;
                    try {
                        srid = dialect.getGeometrySRID(databaseSchema, tableName, name, cx);
                        if(srid != null)
                            crs = dialect.createCRS(srid, cx);
                    } catch (SQLException e) {
                        String msg = "Error occured determing srid for " + tableName + "."+ name;
                        getLogger().log(Level.WARNING, msg, e);
                    }

                    atb.setBinding(binding);
                    atb.setName(ensureGMLNS(namespace,name));
                    atb.setCRS(crs);
                    if(srid != null) adb.addUserData(JDBCDataStore.JDBC_NATIVE_SRID, srid);
                    adb.setName(ensureGMLNS(namespace,name));
                    adb.setType(atb.buildGeometryType());
                    adb.findBestDefaultValue();
                    tb.add(adb.buildDescriptor());
                } else {
                    //add the attribute
                    final Name attName = ensureGMLNS(namespace, name);
                    atb.setName(attName);
                    atb.setBinding(binding);
                    adb.setName(attName);
                    adb.setType(atb.buildType());
                    adb.findBestDefaultValue();
                    tb.add(adb.buildDescriptor());
                }
            }

            return tb.buildSimpleFeatureType();
            
        } catch (SQLException e) {
            throw new DataStoreException("Error occurred building feature type", e);
        }finally {
            closeSafe(columns);
            closeSafe(cx);
        }
    }

    /**
     * Returns the primary key object for a particular entry, deriving it from
     * the underlying database metadata.
     */
    private PrimaryKey createPrimaryKey(final Name entry) throws DataStoreException {

        PrimaryKey pkey;

        Connection cx = null;
        try {
            //get metadata from database
            cx = createConnection();
            final String tableName = entry.getLocalPart();
            final DatabaseMetaData metaData = cx.getMetaData();
            final ResultSet primaryKey = metaData.getPrimaryKeys(null, databaseSchema, tableName);

            try {
                /*
                 *        <LI><B>TABLE_CAT</B> String => table catalog (may be <code>null</code>)
                 *        <LI><B>TABLE_SCHEM</B> String => table schema (may be <code>null</code>)
                 *        <LI><B>TABLE_NAME</B> String => table name
                 *        <LI><B>COLUMN_NAME</B> String => column name
                 *        <LI><B>KEY_SEQ</B> short => sequence number within primary key
                 *        <LI><B>PK_NAME</B> String => primary key name (may be <code>null</code>)
                 */
                final ArrayList<PrimaryKeyColumn> cols = new ArrayList();

                while (primaryKey.next()) {
                    final String columnName = primaryKey.getString(MD_COLUMN_NAME);

                    //look up the type ( should only be one row )
                    final ResultSet columns = metaData.getColumns(null, databaseSchema, tableName, columnName);
                    columns.next();

                    final int binding = columns.getInt(MD_DATA_TYPE);
                    Class columnType = dialect.getMapping(binding);

                    if (columnType == null) {
                        getLogger().warning("No class for sql type " + binding);
                        columnType = Object.class;
                    }

                    //determine which type of primary key we have
                    PrimaryKeyColumn col = null;

                    //1. Auto Incrementing?
                    final Statement st = cx.createStatement();

                    try {
                        //not actually going to get data
                        st.setFetchSize(1);

                        final StringBuilder sql = new StringBuilder();
                        sql.append("SELECT ");
                        dialect.encodeColumnName(columnName, sql);
                        sql.append(" FROM ");
                        queryBuilder.encodeTableName(tableName, sql);

                        sql.append(" WHERE 0=1");

                        getLogger().log(Level.FINE, "Grabbing table pk metadata: {0}", sql);

                        ResultSet rs = null;
                        try {
                            rs = st.executeQuery(sql.toString());

                            if (rs.getMetaData().isAutoIncrement(1)) {
                                col = new AutoGeneratedPrimaryKeyColumn(columnName, columnType);
                            }
                        } finally {
                            closeSafe(rs);
                        }
                    } finally {
                        closeSafe(st);
                    }

                    //2. Has a sequence?
                    if (col == null) {
                        //TODO: look for a sequence
                        final String sequenceName = dialect.getSequenceForColumn(databaseSchema,
                                tableName, columnName, cx);
                        if (sequenceName != null) {
                            col = new SequencedPrimaryKeyColumn(columnName, columnType, sequenceName);
                        }
                    }

                    if (col == null) {
                        col = new NonIncrementingPrimaryKeyColumn(columnName, columnType);
                    }

                    cols.add(col);
                }

                if (cols.isEmpty()) {
                    getLogger().info("No primary key found for " + tableName + ".");
                    pkey = new NullPrimaryKey(tableName);
                } else {
                    pkey = new PrimaryKey(tableName, cols);
                }

            } finally {
                closeSafe(primaryKey);
            }
        } catch (SQLException e) {
            throw new DataStoreException("Error looking up primary key", e);
        } finally {
            closeSafe(cx);
        }

        return pkey;
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public QueryCapabilities getQueryCapabilities() {
        return capabilities;
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public FeatureReader getFeatureReader(Query query) throws DataStoreException {

        final Source source = query.getSource();

        final FeatureReader reader;
        if(source instanceof Join){
            reader = getCrossFeatureReader(query);
        }else if(source instanceof Selector){
            reader = getSimpleFeatureReader(query);
        }else{
            throw new DataStoreException("Unexpected source type : " + source);
        }

        //take care of potential hints, like removing primary keys
        final QueryBuilder qb = new QueryBuilder();
        qb.setTypeName(new DefaultName("remaining"));
        qb.setHints(query.getHints());
        qb.setResolution(query.getResolution());
        return handleRemaining(reader, qb.buildQuery());
    }

    /**
     * Generate a reader from a join query.
     *
     * @param query
     * @return FeatureReader
     * @throws DataStoreException
     */
    private FeatureReader getCrossFeatureReader(Query query) throws DataStoreException {
        /*
         * Query should look like :
          SELECT * FROM
          (SELECT * FROM (SELECT "id","version","userId","timestamp","changeset",encode(asBinary(force_2d("geometry"),'XDR'),'base64') as "geometry" FROM "Way") as l
          INNER JOIN
          (SELECT "wayId","k","v" FROM "WayTag") as r ON l."id" = r."wayId") as l
          LEFT JOIN
          (SELECT "wayId","nodeId","index" FROM "WayMember") as r ON l."id" = r."wayId"
         */

        final LinkedHashMap<String, List<AttributeDescriptor>> atts = new LinkedHashMap<String, List<AttributeDescriptor>>();
        final List<PrimaryKey> pkeys = new ArrayList<PrimaryKey>();

        //build the sql query
        final StringBuilder querySQL = new StringBuilder();
        prepareSelect(query.getSource(), querySQL, atts, pkeys, query.getHints());
        final String sql = querySQL.toString();

        //build the new feature type
        final FeatureTypeBuilder sftb = new FeatureTypeBuilder();
        final AttributeDescriptorBuilder adb = new AttributeDescriptorBuilder();
        sftb.setName(getNamespaceURI(),"crossQuery");

        for(Entry<String,List<AttributeDescriptor>> entry : atts.entrySet()){
            final String selectorName = entry.getKey();
            for(AttributeDescriptor desc : entry.getValue()){
                adb.reset();
                adb.copy(desc);
                final Name oldName = adb.getName();
                adb.setName(new DefaultName(oldName.getNamespaceURI()+"-"+selectorName, oldName.getLocalPart()));
                sftb.add(adb.buildDescriptor());
            }
        }
        final SimpleFeatureType querySchema = sftb.buildSimpleFeatureType();

        final List<PrimaryKeyColumn> columns = new ArrayList<PrimaryKeyColumn>();
        for(PrimaryKey pkey : pkeys){
            columns.addAll(pkey.getColumns());
        }
        final PrimaryKey pkCombine = new PrimaryKey("crossKey", columns);

        //grab connection
        final Connection cx;
        try {
            cx = createConnection();
        } catch (SQLException ex) {
            throw new DataStoreException(ex);
        }

        //create the reader
        FeatureReader<SimpleFeatureType, SimpleFeature> reader;

        try {
            // this allows PostGIS to page the results and respect the fetch size
            // if (getState().getTransaction() == Transaction.AUTO_COMMIT)
            cx.setAutoCommit(false);

            getLogger().fine(sql);

            reader = new JDBCFeatureReader( sql, cx, this, querySchema.getName(), querySchema, pkCombine, query.getHints() );
        } catch (SQLException e) {
            // close the connection
            closeSafe(cx);
            // safely rethrow
            throw new DataStoreException(e);
        } catch (IOException e) {
            // close the connection
            closeSafe(cx);
            // safely rethrow
            throw new DataStoreException(e);
        }

        final Filter filter = query.getFilter();
        if(filter != Filter.INCLUDE){
            reader = GenericFilterFeatureIterator.wrap(reader, filter);
        }

        return reader;
    }

    private void prepareSelect(Source source, StringBuilder sql, LinkedHashMap<String, List<AttributeDescriptor>> att,
            List<PrimaryKey> pkeys, Hints hints) throws DataStoreException{
        if(source instanceof Join){
            prepareSelect((Join)source, sql, att, pkeys, hints);
        }else if(source instanceof Selector){
            prepareSelect((Selector)source, sql, att, pkeys, hints);
        }else{
            throw new IllegalArgumentException("Unknowned source type : "+ source);
        }
    }

    private void prepareSelect(Join source, StringBuilder sql, LinkedHashMap<String, List<AttributeDescriptor>> att,
            List<PrimaryKey> pkeys, Hints hints) throws DataStoreException{

        final Source leftSource = source.getLeft();
        final Source rightSource = source.getRight();
        final PropertyName leftProp = (PropertyName) source.getJoinCondition().getExpression1();
        final PropertyName rightProp = (PropertyName) source.getJoinCondition().getExpression2();

        sql.append("SELECT * FROM (");
        prepareSelect(leftSource, sql, att, pkeys, hints);
        sql.append(") as l ");
        switch(source.getJoinType()){
            case INNER :        sql.append("INNER");break;
            case LEFT_OUTER :   sql.append("LEFT");break;
            case RIGHT_OUTER :  sql.append("RIGHT");break;
        }
        sql.append(" JOIN (");
        prepareSelect(rightSource, sql, att, pkeys, hints);
        sql.append(") as r ON l.");
        dialect.encodeColumnName(DefaultName.valueOf(leftProp.getPropertyName()).getLocalPart(), sql);
        sql.append(" = r.");
        dialect.encodeColumnName(DefaultName.valueOf(rightProp.getPropertyName()).getLocalPart(), sql);
    }

    private void prepareSelect(Selector source, StringBuilder sql, LinkedHashMap<String, List<AttributeDescriptor>> att,
            List<PrimaryKey> pkeys, Hints hints) throws DataStoreException{
        final SimpleFeatureType type = (SimpleFeatureType) getFeatureType(source.getFeatureTypeName());
        final PrimaryKey pk = getPrimaryKey(type.getName());

        sql.append("SELECT ");

        final List<AttributeDescriptor> descs = type.getAttributeDescriptors();
        for(AttributeDescriptor desc : descs){
            if (desc instanceof GeometryDescriptor) {
                queryBuilder.encodeGeometryColumn((GeometryDescriptor) desc, sql, hints);
                dialect.encodeColumnAlias(desc.getLocalName(), sql);
            }else{
                dialect.encodeColumnName(desc.getLocalName(), sql);
            }
            sql.append(',');
        }

        sql.setLength(sql.length()-1);
        sql.append(" FROM ");
        dialect.encodeTableName(type.getTypeName(), sql);

        att.put(source.getSelectorName(), descs);
        pkeys.add(pk);
    }

    private FeatureReader getSimpleFeatureReader(Query query) throws DataStoreException {
        final SimpleFeatureType type = (SimpleFeatureType) getFeatureType(query.getTypeName());
        final PrimaryKey pkey = getPrimaryKey(query.getTypeName());

        // split the filter
        final Filter[] split = splitFilter(query.getFilter(),type);
        final Filter preFilter = split[0];
        final Filter postFilter = split[1];

        // rebuild a new query with the same params, but just the pre-filter
        final QueryBuilder builder = new QueryBuilder(query);
        builder.setFilter(preFilter);
        final Query preQuery = builder.buildQuery();

        // Build the feature type returned by this query. Also build an eventual extra feature type
        // containing the attributes we might need in order to evaluate the post filter
        SimpleFeatureType querySchema;
        SimpleFeatureType returnedSchema;
        if(query.retrieveAllProperties()) {
            returnedSchema = querySchema = type;
        } else {
            returnedSchema = (SimpleFeatureType)FeatureTypeBuilder.retype(type, query.getPropertyNames());
            final FilterAttributeExtractor extractor = new FilterAttributeExtractor(type);
            postFilter.accept(extractor, null);
            final Name[] extraAttributes = extractor.getAttributeNames();
            final List<Name> allAttributes = new ArrayList<Name>(Arrays.asList(query.getPropertyNames()));
            for (Name extraAttribute : extraAttributes) {
                if(!allAttributes.contains(extraAttribute)) {
                    allAttributes.add(extraAttribute);
                }
            }

            //ensure we have the primarykeys
            pkLoop :
            for(PrimaryKeyColumn pkc : pkey.getColumns()){
                final String pkcName = pkc.getName();
                for(Name n : allAttributes){
                    if(n.getLocalPart().equals(pkcName)){
                        continue pkLoop;
                     }
                 }
                //add the pk attribut
                allAttributes.add(type.getDescriptor(pkcName).getName());
             }

            final Name[] allAttributeArray = allAttributes.toArray(new Name[allAttributes.size()]);
            querySchema = (SimpleFeatureType)FeatureTypeBuilder.retype(type, allAttributeArray);
        }

        //grab connection
        final Connection cx;
        try {
            cx = createConnection();
        } catch (SQLException ex) {
            throw new DataStoreException(ex);
        }

        //create the reader
        FeatureReader<SimpleFeatureType, SimpleFeature> reader;

        try {
            // this allows PostGIS to page the results and respect the fetch size
            // if (getState().getTransaction() == Transaction.AUTO_COMMIT)
            cx.setAutoCommit(false);

            final SQLDialect dialect = getDialect();
            if (dialect instanceof PreparedStatementSQLDialect) {
                final PreparedStatement ps = queryBuilder.selectSQLPS(querySchema, preQuery, cx);
                reader = new JDBCFeatureReader(ps, cx, this, query.getTypeName(), querySchema, pkey, query.getHints());
            } else {
                //build up a statement for the content
                final String sql = queryBuilder.selectSQL(querySchema, preQuery);
                getLogger().fine(sql);

                reader = new JDBCFeatureReader( sql, cx, this, query.getTypeName(), querySchema, pkey, query.getHints() );
            }
        } catch (SQLException e) {
            // close the connection
            closeSafe(cx);
            // safely rethrow
            throw new DataStoreException(e);
        } catch (IOException e) {
            // close the connection
            closeSafe(cx);
            // safely rethrow
            throw new DataStoreException(e);
        }


        // if post filter, wrap it
        if (postFilter != null && postFilter != Filter.INCLUDE) {
            reader = GenericFilterFeatureIterator.wrap(reader, postFilter);
            if(!returnedSchema.equals(querySchema))
                reader = GenericRetypeFeatureIterator.wrap(reader, returnedSchema,query.getHints());
        }

        //if we need to reproject data
        final CoordinateReferenceSystem reproject = query.getCoordinateSystemReproject();
        if(reproject != null && !CRS.equalsIgnoreMetadata(reproject,type.getCoordinateReferenceSystem())){
            try {
                reader = GenericReprojectFeatureIterator.wrap(reader, reproject,query.getHints());
            } catch (FactoryException ex) {
                throw new DataStoreException(ex);
            } catch (SchemaException ex) {
                throw new DataStoreException(ex);
            }
        }

        //if we need to constraint type
        if(!returnedSchema.equals(querySchema)){
            reader = GenericRetypeFeatureIterator.wrap(reader, returnedSchema, query.getHints());
        }

        return reader;
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public FeatureWriter getFeatureWriter(Name typeName, Filter filter) throws DataStoreException {
        try {
            return getFeatureWriterInternal(typeName, filter, WRITER_ADD | WRITER_UPDATE);
        } catch (IOException ex) {
            throw new DataStoreException(ex);
        }
    }

    @Override
    public FeatureWriter getFeatureWriterAppend(Name typeName) throws DataStoreException {
        try {
            return getFeatureWriterInternal(typeName, Filter.EXCLUDE, WRITER_ADD);
        } catch (IOException ex) {
            throw new DataStoreException(ex);
        }
    }

    private FeatureWriter getFeatureWriterInternal(final Name typeName, final Filter filter, final int flags)
            throws DataStoreException, IOException {

        if(!isWritable(typeName)){
            throw new DataStoreException("Type "+ typeName + " is not writeable.");
        }
        if (flags == 0) {
            throw new IllegalArgumentException( "no write flags set" );
        }

        final SimpleFeatureType type = (SimpleFeatureType) getFeatureType(typeName);

        //split the filter
        final Filter[] split = splitFilter(filter,type);


        Connection cx = null;
        FeatureWriter<SimpleFeatureType, SimpleFeature> writer;
        try {
            final PrimaryKey pkey = getPrimaryKey(typeName);
            cx = createConnection();
            //check for insert only
            if ( (flags | WRITER_ADD) == WRITER_ADD ) {
                //build up a statement for the content, inserting only so we dont want
                //the query to return any data ==> Filter.EXCLUDE
                final Query queryNone = QueryBuilder.filtered(typeName, Filter.EXCLUDE);
                if ( getDialect() instanceof PreparedStatementSQLDialect ) {
                    final PreparedStatement ps = queryBuilder.selectSQLPS(type, queryNone, cx);
                    return new JDBCInsertFeatureWriter( ps, cx, this, typeName, type, pkey, null);
                }else{
                    final String sql = queryBuilder.selectSQL(type, queryNone);
                    getLogger().fine(sql);
                    return new JDBCInsertFeatureWriter( sql, cx, this, typeName, type, pkey, null);
                }
            }


            // build up a statement for the content
            final Query preQuery = QueryBuilder.filtered(typeName, split[0]);

            if(getDialect() instanceof PreparedStatementSQLDialect) {
                final PreparedStatement ps = queryBuilder.selectSQLPS(type, preQuery, cx);
                if ( (flags | WRITER_UPDATE) == WRITER_UPDATE ) {
                    writer = new JDBCUpdateFeatureWriter(ps, cx, this, typeName, type, pkey, null);
                } else {
                    //update insert case
                    writer = new JDBCUpdateInsertFeatureWriter(ps, cx, this, typeName, type, pkey, null, null);
                }
            } else {
                final String sql = queryBuilder.selectSQL(type, preQuery);
                getLogger().fine(sql);

                if ( (flags | WRITER_UPDATE) == WRITER_UPDATE ) {
                    writer = new JDBCUpdateFeatureWriter( sql, cx, this, typeName, type, pkey, null);
                } else {
                    //update insert case
                    writer = new JDBCUpdateInsertFeatureWriter( sql, cx, this, typeName, type, pkey, null);
                }
            }

        } catch (SQLException e) {
            // close the connection
            closeSafe(cx);
            // now we can safely rethrow the exception
            throw new DataStoreException(e);
        }

        //check for post filter and wrap accordingly
        if ( split[1] != null && split[1] != Filter.INCLUDE ) {
            writer = GenericFilterFeatureIterator.wrap(writer, split[1]);
        }
        return writer;
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public long getCount(Query query) throws DataStoreException {
        typeCheck(query.getTypeName());
        final FeatureType type = getFeatureType(query.getTypeName());

        //split the filter
        final Filter[] split = splitFilter( query.getFilter(),type );
        final Filter preFilter = split[0];
        final Filter postFilter = split[1];


        if ((postFilter != null) && (postFilter != Filter.INCLUDE)) {
            try {
                //calculate manually, dont use datastore optimization
                getLogger().fine("Calculating size manually");
                return DataUtilities.calculateCount(getFeatureReader(query));
            } catch (Exception e) {
                throw new RuntimeException(e);
            }
        } else {
            //no post filter, we have a preFilter, or preFilter is null..
            // either way we can use the datastore optimization
            final Connection cx;
            try {
                cx = createConnection();
            } catch (SQLException ex) {
                throw new DataStoreException(ex);
            }
            try {
                final QueryBuilder builder = new QueryBuilder(query);
                builder.setFilter(preFilter);
                final Query q = builder.buildQuery();
                int count = getCount(type, q, cx);
                // if native support for limit and offset is not implemented, we have to ajust the result
                if (!getDialect().isLimitOffsetSupported()) {
                    if (query.getStartIndex() > 0) {
                        if (query.getStartIndex() > count) {
                            count = 0;
                        } else {
                            count -= query.getStartIndex();
                        }
                    }
                    if (query.getMaxFeatures() > 0 && count > query.getMaxFeatures()) {
                        count = query.getMaxFeatures();
                    }
                }
                return count;
            } finally {
                closeSafe(cx);
            }
        }
    }

    /**
     * Returns the count of the features for a particular feature type / table.
     * Rely on the database count capabilities.
     */
    private int getCount(final FeatureType featureType, final Query query, final Connection cx)
            throws DataStoreException {

        final Statement st;
        final ResultSet rs;
        try {
            if (dialect instanceof PreparedStatementSQLDialect) {
                st = queryBuilder.selectCountSQLPS(featureType, query, cx);
                rs = ((PreparedStatement) st).executeQuery();
            } else {
                final String sql = queryBuilder.selectCountSQL(featureType, query);
                getLogger().log(Level.FINE, "Counting features: {0}", sql);

                st = cx.createStatement();
                rs = st.executeQuery(sql);
            }

            try {
                rs.next();
                return rs.getInt(1);
            } finally {
                closeSafe(rs);
                closeSafe(st);
            }
        } catch (SQLException e) {
            throw new DataStoreException("Error occured calculating count", e);
        }
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public Envelope getEnvelope(Query query) throws DataStoreException, DataStoreRuntimeException {
        typeCheck(query.getTypeName());
        final SimpleFeatureType type = (SimpleFeatureType) getFeatureType(query.getTypeName());

        //split the filter
        final Filter[] split = splitFilter(query.getFilter(),type);
        final Filter preFilter = split[0];
        final Filter postFilter = split[1];

        final boolean canLimitOffset = dialect.isLimitOffsetSupported();

        if ((postFilter != null) && (postFilter != Filter.INCLUDE)
                || (query.getMaxFeatures() != null && !canLimitOffset)
                || (query.getStartIndex() > 0 && !canLimitOffset) ){
            //calculate manually, don't use datastore optimization
            getLogger().fine("Calculating bounds manually");

            // grab a reader
            final QueryBuilder builder = new QueryBuilder(query);
            builder.setFilter(postFilter);
            return DataUtilities.calculateEnvelope(getFeatureReader(builder.buildQuery()));
        } else {
            //post filter was null... pre can be set or null... either way
            // use datastore optimization
            final QueryBuilder builder = new QueryBuilder(query);
            builder.setFilter(preFilter);
            return getEnvelope(type, builder.buildQuery());
        }
    }

    /**
     * Returns the bounds of the features for a particular feature type / table.
     * Rely on tha database to obtain envelope.
     *
     * @param featureType The feature type / table.
     * @param query Specifies rows to include in bounds calculation, as well as how many
     *              features and the offset if needed
     */
    private Envelope getEnvelope(final FeatureType featureType, final Query query)
            throws DataStoreException {

        // handle geometryless case by returning a null envelope
        if (featureType.getGeometryDescriptor() == null) {
            return null;
        }

        Connection cx = null;
        Statement st = null;
        ResultSet rs = null;
        try {
            cx = createConnection();
            if (dialect instanceof PreparedStatementSQLDialect) {
                st = queryBuilder.selectBoundsSQLPS(featureType, query, cx);
                rs = ((PreparedStatement) st).executeQuery();
            } else {
                final String sql = queryBuilder.selectBoundsSQL(featureType, query);
                getLogger().log(Level.FINE, "Retriving bounding box: {0}", sql);
                st = cx.createStatement();
                rs = st.executeQuery(sql);
            }

            JTSEnvelope2D bbox = null;

            while(rs.next()){
                try {
                    final com.vividsolutions.jts.geom.Envelope e =
                            dialect.decodeGeometryEnvelope(rs, 1, st.getConnection());

                    if(e !=  null){
                        if(bbox == null){
                            final CoordinateReferenceSystem flatCRS = CRS.getHorizontalCRS(
                                featureType.getCoordinateReferenceSystem());
                            bbox = new JTSEnvelope2D(e, flatCRS);
                        }else{
                            bbox.expandToInclude(e);
                        }
                    }

                } catch (IOException ex) {
                    throw new DataStoreException(ex);
                }
            }
            
            return bbox;
        } catch (SQLException e) {
            throw new DataStoreException("Error occured calculating bounds", e);
        } finally {
            closeSafe(cx, st, rs);
        }
    }

    /**
     * Inserts a collection of new features into the database for a particular
     * feature type / table.
     */
    protected void insert(final Collection<? extends Feature> features, final SimpleFeatureType featureType,
            final Connection cx) throws DataStoreException {
        final PrimaryKey key = getPrimaryKey(featureType.getName());

        // we do this in a synchronized block because we need to do two queries,
        // first to figure out what the id will be, then the insert statement
        synchronized (this) {
            Statement st = null;
            Iterator<? extends Feature> ite = null;
            try {
                if (!(dialect instanceof PreparedStatementSQLDialect)) {
                    st = cx.createStatement();
                }

                //figure out what the next fid will be
                final List<Object> nextKeyValues = getNextValues(key, cx);
                ite = features.iterator();

                while(ite.hasNext()){
                    final SimpleFeature feature = (SimpleFeature) ite.next();

                    if (dialect instanceof PreparedStatementSQLDialect) {
                        final PreparedStatement ps = queryBuilder.insertSQLPS(featureType, feature, nextKeyValues, cx);
                        try {
                            ps.execute();
                        } finally {
                            closeSafe(ps);
                        }
                    } else {
                        //this technic must be generalize to all primary keys, must revisite tests for this
                        final String sql = queryBuilder.insertSQL(featureType, feature, nextKeyValues, cx);
                        getLogger().log(Level.FINE, "Inserting new feature: {0}", sql);

                        if(nextKeyValues.isEmpty() || nextKeyValues.get(0) == null){
                            st.execute(sql,Statement.RETURN_GENERATED_KEYS);
                            ResultSet rs = st.getGeneratedKeys();
                            rs.next();
                            final int id = rs.getInt(1);
                            nextKeyValues.set(0, id);
                            rs.close();
                            feature.setAttribute(key.getColumns().get(0).getName(), id);
                        }else{
                            st.execute(sql);
                        }
                    }

                    //report the feature id as user data since we cant set the fid
                    final String fid = featureType.getTypeName() + "." + PrimaryKey.encodeFID(nextKeyValues);
                    feature.getUserData().put("fid", fid);
                }

            //st.executeBatch();
            } catch (SQLException e) {
                throw new DataStoreException("Error inserting features",e);
            } catch (IOException e) {
                throw new DataStoreException("Error inserting features",e);
            }finally {
                closeSafe(st);

                if(ite != null && ite instanceof Closeable){
                    try {
                        ((Closeable) ite).close();
                    } catch (IOException ex) {
                        getLogger().log(Level.FINE, "Failed to close collection iterator.", ex);
                    }
                }

            }
        }
    }

    /**
     * Updates an existing feature(s) in the database for a particular feature type / table.
     */
    protected void update(final SimpleFeatureType featureType, final Map<AttributeDescriptor,Object> changes,
            final Filter filter, final Connection cx) throws DataStoreException{
        if ((changes == null) || (changes.isEmpty())) {
            getLogger().warning("Update called with no attributes, doing nothing.");
            return;
        }

        if (dialect instanceof PreparedStatementSQLDialect) {
            PreparedStatement ps = null;
            try {
                ps = queryBuilder.updateSQLPS(featureType, changes, filter, cx);
                ps.execute();
            }catch (SQLException e) {
                throw new DataStoreException("Error occured updating features",e);
            }finally {
                closeSafe(ps);
            }
        } else {
            Statement st = null;
            try {
                final String sql = queryBuilder.updateSQL(featureType, changes, filter);
                getLogger().log(Level.FINE, "Updating feature: {0}", sql);
                st = cx.createStatement();
                st.execute(sql);
            } catch (SQLException e) {
                throw new DataStoreException("Error occured updating features",e);
            } catch (IOException e) {
                throw new DataStoreException("Error occured updating features",e);
            } finally {
                closeSafe(st);
            }
        }
    }

    /**
     * Deletes an existing feature(s) in the database for a particular feature type / table.
     */
    protected void delete(final SimpleFeatureType featureType, final Filter filter, final Connection cx)
            throws IOException {

        Statement st = null;
        try {
            if (dialect instanceof PreparedStatementSQLDialect) {
                st = queryBuilder.deleteSQLPS(featureType, filter, cx);
                ((PreparedStatement) st).execute();
            } else {
                final String sql = queryBuilder.deleteSQL(featureType, filter);
                getLogger().log(Level.FINE, "Removing feature(s): {0}", sql);
                st = cx.createStatement();
                st.execute(sql);
            }
        } catch (SQLException e) {
            throw new IOException("Error occured calculating bounds", e);
        } finally {
            closeSafe(st);
        }
    }


    ////////////////////////////////////////////////////////////////////////////
    // schema manipulation /////////////////////////////////////////////////////
    ////////////////////////////////////////////////////////////////////////////

    /**
     * {@inheritDoc }
     */
    @Override
    public void createSchema(Name typeName, FeatureType featureType) throws DataStoreException {

        if(typeName == null){
            throw new DataStoreException("Type name can not be null.");
        }

        if(!(featureType instanceof SimpleFeatureType)){
            throw new DataStoreException("JDBC datastore can handle only simple feature types.");
        }

        if(!featureType.getName().equals(typeName)){
            throw new DataStoreException("JDBC datastore can only hold typename same as feature type name.");
        }

        if(getNames().contains(typeName)){
            throw new DataStoreException("Type name "+ typeName + " already exists.");
        }


        //execute the create table statement
        //TODO: create a primary key and a spatial index
        Connection cx = null;

        try {
            cx = createConnection();
            final String sql = queryBuilder.createTableSQL((SimpleFeatureType) featureType,cx);
            getLogger().log(Level.FINE, "Create schema: {0}", sql);

            final Statement st = cx.createStatement();

            try {
                st.execute(sql);
            } finally {
                closeSafe(st);
            }

            dialect.postCreateTable(databaseSchema, (SimpleFeatureType)featureType, cx);
        } catch (Exception e) {
            throw new DataStoreException("Error occurred creating table", e);
        } finally {
            closeSafe(cx);
        }

        // reset the type name cache, will be recreated when needed.
        nameCache = null;
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public void updateSchema(Name typeName, FeatureType featureType) throws DataStoreException {
        throw new UnsupportedOperationException("Not supported.");
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public void deleteSchema(Name typeName) throws DataStoreException {
        throw new UnsupportedOperationException("Not supported.");
    }


    ////////////////////////////////////////////////////////////////////////////
    // SQL utils ///////////////////////////////////////////////////////////////
    ////////////////////////////////////////////////////////////////////////////

    /**
     * Helper method for splitting a filter.
     */
    private Filter[] splitFilter(final Filter original, FeatureType schema) throws DataStoreException {
        final Filter[] split = new Filter[2];
        if ( original != null ) {
            //create a filter splitter
            final CapabilitiesFilterSplitter splitter = new CapabilitiesFilterSplitter(
                    getFilterCapabilities(), schema);
            original.accept(splitter, null);

            split[0] = splitter.getPreFilter();
            split[1] = splitter.getPostFilter();
        }

        final SimplifyingFilterVisitor visitor = new SimplifyingFilterVisitor();
        final PrimaryKey key = getPrimaryKey(schema.getName());
        visitor.setFIDValidator( new PrimaryKeyFIDValidator( this,key ) );
        split[0] = (Filter) split[0].accept(visitor, null);
        split[1] = (Filter) split[1].accept(visitor, null);

        return split;
    }

    /**
     * The filter capabilities which reports which spatial operations the
     * underlying database can handle natively.
     *
     * @return The filter capabilities, never <code>null</code>.
     */
    private DefaultFilterCapabilities getFilterCapabilities() {
        if (dialect instanceof PreparedStatementSQLDialect) {
            return ((PreparedStatementSQLDialect) dialect).createPreparedFilterToSQL().getCapabilities();
        } else {
            return dialect.createFilterToSQL().getCapabilities();
        }
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public FilterToSQL createFilterToSQL(final FeatureType featureType) {
        return queryBuilder.initializeFilterToSQL(dialect.createFilterToSQL(), featureType);
    }

    /**
     * Gets the next value of a primary key.
     */
    protected List<Object> getNextValues(final PrimaryKey pkey, final Connection cx)
            throws SQLException, IOException {
        final ArrayList<Object> next = new ArrayList<Object>();
        for (PrimaryKeyColumn col : pkey.getColumns()) {
            next.add(getNextValue(col, pkey, cx));
        }
        return next;
    }

    /**
     * Gets the next value for the column of a primary key.
     */
    protected Object getNextValue(final PrimaryKeyColumn col, final PrimaryKey pkey, final Connection cx)
            throws SQLException, IOException {
        Object next = null;

        if (col instanceof AutoGeneratedPrimaryKeyColumn) {
            next = dialect.getNextAutoGeneratedValue(databaseSchema, pkey.getTableName(), col.getName(), cx);
        } else if (col instanceof SequencedPrimaryKeyColumn) {
            final String sequenceName = ((SequencedPrimaryKeyColumn) col).getSequenceName();
            next = dialect.getNextSequenceValue(databaseSchema, sequenceName, cx);
        } else {
            //try to calculate
            final Class t = col.getType();

            //is the column numeric?
            if (Number.class.isAssignableFrom(t)) {

                //search the max value.
                final StringBuilder sql = new StringBuilder();
                sql.append("SELECT MAX(");
                dialect.encodeColumnName(col.getName(), sql);
                sql.append(") FROM ");
                queryBuilder.encodeTableName(pkey.getTableName(), sql);
                getLogger().log(Level.FINE, "Getting next FID: {0}", sql);

                final Statement st = cx.createStatement();
                ResultSet rs = null;
                try {
                    rs = st.executeQuery(sql.toString());
                    rs.next();
                    next = rs.getObject(1);
                } finally {
                    closeSafe(rs);
                    closeSafe(st);
                }

                if(next == null){
                    //this probably means there was no data in the table, set to 1
                    //TODO: probably better to do a count to check... but if this
                    // value already exists the db will throw an error when it tries
                    // to insert
                    next = 1;
                }else if (t == Short.class || t == Integer.class || t == Long.class
                        || BigInteger.class.isAssignableFrom(t)
                        || BigDecimal.class.isAssignableFrom(t) ) {
                    next = ((Number)next).longValue() +1;
                }else if (t == Float.class){
                    next = Math.nextUp( ((Number)next).floatValue() );
                }else if (t == Double.class){
                    next = Math.nextUp( ((Number)next).doubleValue() );
                }else{
                    //can't calculate for other types
                    next = 1;
                }

            } else if (CharSequence.class.isAssignableFrom(t)) {
                //generate a random string
                next = SimpleFeatureBuilder.createDefaultFeatureId();
            }

            if (next == null) {
                throw new IOException("Cannot generate key value for column of type: " + t.getName());
            }
        }

        return next;
    }


    ////////////////////////////////////////////////////////////////////////////
    // Fallback on reader/write iterator methods ///////////////////////////////
    ////////////////////////////////////////////////////////////////////////////

    /**
     * {@inheritDoc }
     */
    @Override
    public List<FeatureId> addFeatures(Name groupName, Collection<? extends Feature> newFeatures) throws DataStoreException {
        return handleAddWithFeatureWriter(groupName, newFeatures);
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public void updateFeatures(Name groupName, Filter filter, Map<? extends PropertyDescriptor, ? extends Object> values) throws DataStoreException {
        handleUpdateWithFeatureWriter(groupName, filter, values);
    }

    /**
     * {@inheritDoc }
     */
    @Override
    public void removeFeatures(Name groupName, Filter filter) throws DataStoreException {
        handleRemoveWithFeatureWriter(groupName, filter);
    }


    ////////////////////////////////////////////////////////////////////////////
    // other utils /////////////////////////////////////////////////////////////
    ////////////////////////////////////////////////////////////////////////////

    @Override
    protected void finalize() throws Throwable {
        if (source != null) {
            getLogger().severe("There's code using JDBC based datastore and " +
                    "not disposing them. This may lead to temporary loss of database connections. " +
                    "Please make sure all data access code calls DataStore.dispose() " +
                    "before freeing all references to it");
            dispose();
        }
        super.finalize();
    }

    @Override
    public void dispose() {
        if (source instanceof ManageableDataSource) {
            try {
                final ManageableDataSource mds = (ManageableDataSource) source;
                source = null;
                mds.close();
            } catch (SQLException e) {
                // it's ok, we did our best..
                getLogger().log(Level.FINE, "Could not close dataSource", e);
            }
        }
    }

}